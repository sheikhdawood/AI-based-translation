# app.py  â€”â€” Arabic â English Translator (Gradio)
print("ğŸ“¢ app.py started")

import gradio as gr
import torch, os, psutil
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from peft import PeftModel, PeftConfig

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Utils â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_memory_usage_mb() -> float:
    """Return current RAM usage of this process in MB."""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / (1024 * 1024)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Model load (oneâ€‘time) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
print("ğŸ”„ Loading model + adapter â€¦")
adapter_path = "adapters/ar_en/"
config       = PeftConfig.from_pretrained(adapter_path)

device       = "cuda" if torch.cuda.is_available() else "cpu"

base_model   = AutoModelForSeq2SeqLM.from_pretrained(
    config.base_model_name_or_path,
    torch_dtype = torch.float16 if device == "cuda" else None,
).to(device)

model        = PeftModel.from_pretrained(base_model, adapter_path).to(device)
tokenizer    = AutoTokenizer.from_pretrained(config.base_model_name_or_path)

print(f"âœ… Model ready (RAM: {get_memory_usage_mb():.2f}â€¯MB)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Translation fn â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate(text: str) -> str:
    text = text.strip()
    if not text:
        return "âš ï¸â€¯Please enter some Arabic text."
    
    inputs  = tokenizer(text, return_tensors="pt", truncation=True).to(device)
    with torch.no_grad():
        output_ids = model.generate(**inputs, max_new_tokens=128)
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradio UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
demo = gr.Interface(
    fn          = translate,
    inputs      = gr.Textbox(label="ğŸ“ Enter Arabic text"),
    outputs     = gr.Textbox(label="ğŸ”¤ Translated Text (English)"),
    title       = "ğŸŒ Arabic â English Translator",
    description = f"Model loaded Â· RAM usage {get_memory_usage_mb():.1f}â€¯MB",
    examples    = ["Ø¨ÙØ³Ù’Ù…Ù Ø§Ù„Ù„Ù‘ÙÙ‡Ù Ø§Ù„Ø±Ù‘ÙØ­Ù’Ù…ÙÙ°Ù†Ù Ø§Ù„Ø±Ù‘ÙØ­ÙÙŠÙ…Ù", "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡"]
)

if __name__ == "__main__":
    demo.launch()
