{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 7558,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0026464653147639686,
      "grad_norm": 0.30853238701820374,
      "learning_rate": 0.0001997618417570786,
      "loss": 2.3402,
      "step": 10
    },
    {
      "epoch": 0.005292930629527937,
      "grad_norm": 0.3894115388393402,
      "learning_rate": 0.0001994972214871659,
      "loss": 2.224,
      "step": 20
    },
    {
      "epoch": 0.007939395944291905,
      "grad_norm": 0.3992874324321747,
      "learning_rate": 0.00019923260121725323,
      "loss": 2.1867,
      "step": 30
    },
    {
      "epoch": 0.010585861259055874,
      "grad_norm": 0.4327486455440521,
      "learning_rate": 0.00019896798094734056,
      "loss": 2.2611,
      "step": 40
    },
    {
      "epoch": 0.013232326573819841,
      "grad_norm": 0.4792965352535248,
      "learning_rate": 0.0001987033606774279,
      "loss": 2.1909,
      "step": 50
    },
    {
      "epoch": 0.01587879188858381,
      "grad_norm": 0.49443018436431885,
      "learning_rate": 0.00019843874040751522,
      "loss": 2.1705,
      "step": 60
    },
    {
      "epoch": 0.01852525720334778,
      "grad_norm": 0.47992581129074097,
      "learning_rate": 0.00019817412013760255,
      "loss": 2.1109,
      "step": 70
    },
    {
      "epoch": 0.02117172251811175,
      "grad_norm": 0.43659406900405884,
      "learning_rate": 0.00019790949986768988,
      "loss": 2.1346,
      "step": 80
    },
    {
      "epoch": 0.023818187832875714,
      "grad_norm": 0.49211642146110535,
      "learning_rate": 0.0001976448795977772,
      "loss": 2.1062,
      "step": 90
    },
    {
      "epoch": 0.026464653147639683,
      "grad_norm": 0.49596497416496277,
      "learning_rate": 0.00019738025932786453,
      "loss": 2.1743,
      "step": 100
    },
    {
      "epoch": 0.029111118462403652,
      "grad_norm": 0.45070624351501465,
      "learning_rate": 0.00019711563905795186,
      "loss": 2.1358,
      "step": 110
    },
    {
      "epoch": 0.03175758377716762,
      "grad_norm": 0.45681899785995483,
      "learning_rate": 0.0001968510187880392,
      "loss": 1.9832,
      "step": 120
    },
    {
      "epoch": 0.034404049091931586,
      "grad_norm": 0.4889923334121704,
      "learning_rate": 0.0001965863985181265,
      "loss": 2.0957,
      "step": 130
    },
    {
      "epoch": 0.03705051440669556,
      "grad_norm": 0.4228292405605316,
      "learning_rate": 0.00019632177824821382,
      "loss": 2.0551,
      "step": 140
    },
    {
      "epoch": 0.039696979721459524,
      "grad_norm": 0.5912954807281494,
      "learning_rate": 0.00019605715797830115,
      "loss": 2.0448,
      "step": 150
    },
    {
      "epoch": 0.0423434450362235,
      "grad_norm": 0.5104663968086243,
      "learning_rate": 0.00019579253770838847,
      "loss": 2.0261,
      "step": 160
    },
    {
      "epoch": 0.04498991035098746,
      "grad_norm": 0.4491906464099884,
      "learning_rate": 0.0001955279174384758,
      "loss": 2.0248,
      "step": 170
    },
    {
      "epoch": 0.04763637566575143,
      "grad_norm": 0.6732021570205688,
      "learning_rate": 0.00019526329716856313,
      "loss": 2.042,
      "step": 180
    },
    {
      "epoch": 0.0502828409805154,
      "grad_norm": 0.5586041808128357,
      "learning_rate": 0.00019499867689865046,
      "loss": 1.9932,
      "step": 190
    },
    {
      "epoch": 0.052929306295279366,
      "grad_norm": 0.6783838868141174,
      "learning_rate": 0.00019473405662873776,
      "loss": 2.004,
      "step": 200
    },
    {
      "epoch": 0.05557577161004334,
      "grad_norm": 0.6285017132759094,
      "learning_rate": 0.0001944694363588251,
      "loss": 1.9605,
      "step": 210
    },
    {
      "epoch": 0.058222236924807304,
      "grad_norm": 0.5499271154403687,
      "learning_rate": 0.00019420481608891242,
      "loss": 2.0841,
      "step": 220
    },
    {
      "epoch": 0.06086870223957127,
      "grad_norm": 0.5955424308776855,
      "learning_rate": 0.00019394019581899974,
      "loss": 2.0569,
      "step": 230
    },
    {
      "epoch": 0.06351516755433524,
      "grad_norm": 0.6012045741081238,
      "learning_rate": 0.00019367557554908707,
      "loss": 2.1024,
      "step": 240
    },
    {
      "epoch": 0.06616163286909921,
      "grad_norm": 0.6087979674339294,
      "learning_rate": 0.0001934109552791744,
      "loss": 1.9796,
      "step": 250
    },
    {
      "epoch": 0.06880809818386317,
      "grad_norm": 0.5842451453208923,
      "learning_rate": 0.0001931463350092617,
      "loss": 2.0022,
      "step": 260
    },
    {
      "epoch": 0.07145456349862715,
      "grad_norm": 0.6548529863357544,
      "learning_rate": 0.00019288171473934903,
      "loss": 1.9834,
      "step": 270
    },
    {
      "epoch": 0.07410102881339112,
      "grad_norm": 0.5064845085144043,
      "learning_rate": 0.00019261709446943636,
      "loss": 1.9883,
      "step": 280
    },
    {
      "epoch": 0.07674749412815508,
      "grad_norm": 0.6032368540763855,
      "learning_rate": 0.0001923524741995237,
      "loss": 1.9322,
      "step": 290
    },
    {
      "epoch": 0.07939395944291905,
      "grad_norm": 0.5244259834289551,
      "learning_rate": 0.00019208785392961102,
      "loss": 2.0177,
      "step": 300
    },
    {
      "epoch": 0.08204042475768301,
      "grad_norm": 0.5464279651641846,
      "learning_rate": 0.00019182323365969834,
      "loss": 1.9031,
      "step": 310
    },
    {
      "epoch": 0.084686890072447,
      "grad_norm": 0.5299912691116333,
      "learning_rate": 0.00019155861338978567,
      "loss": 1.8825,
      "step": 320
    },
    {
      "epoch": 0.08733335538721096,
      "grad_norm": 0.6415929198265076,
      "learning_rate": 0.000191293993119873,
      "loss": 2.0321,
      "step": 330
    },
    {
      "epoch": 0.08997982070197492,
      "grad_norm": 0.6040701866149902,
      "learning_rate": 0.00019102937284996033,
      "loss": 2.029,
      "step": 340
    },
    {
      "epoch": 0.09262628601673889,
      "grad_norm": 0.7219608426094055,
      "learning_rate": 0.00019076475258004766,
      "loss": 2.0181,
      "step": 350
    },
    {
      "epoch": 0.09527275133150286,
      "grad_norm": 0.6083056330680847,
      "learning_rate": 0.00019050013231013498,
      "loss": 1.9345,
      "step": 360
    },
    {
      "epoch": 0.09791921664626684,
      "grad_norm": 0.7181945443153381,
      "learning_rate": 0.00019023551204022229,
      "loss": 1.9317,
      "step": 370
    },
    {
      "epoch": 0.1005656819610308,
      "grad_norm": 0.5936657190322876,
      "learning_rate": 0.00018997089177030961,
      "loss": 2.0485,
      "step": 380
    },
    {
      "epoch": 0.10321214727579477,
      "grad_norm": 0.6285514235496521,
      "learning_rate": 0.00018970627150039694,
      "loss": 1.9106,
      "step": 390
    },
    {
      "epoch": 0.10585861259055873,
      "grad_norm": 0.7637150883674622,
      "learning_rate": 0.00018944165123048427,
      "loss": 1.9243,
      "step": 400
    },
    {
      "epoch": 0.1085050779053227,
      "grad_norm": 0.793474018573761,
      "learning_rate": 0.0001891770309605716,
      "loss": 1.9986,
      "step": 410
    },
    {
      "epoch": 0.11115154322008668,
      "grad_norm": 0.6250340938568115,
      "learning_rate": 0.0001889124106906589,
      "loss": 1.8466,
      "step": 420
    },
    {
      "epoch": 0.11379800853485064,
      "grad_norm": 0.5054340362548828,
      "learning_rate": 0.00018864779042074623,
      "loss": 1.9093,
      "step": 430
    },
    {
      "epoch": 0.11644447384961461,
      "grad_norm": 0.5450533628463745,
      "learning_rate": 0.00018838317015083356,
      "loss": 1.9849,
      "step": 440
    },
    {
      "epoch": 0.11909093916437857,
      "grad_norm": 0.5361204147338867,
      "learning_rate": 0.00018811854988092088,
      "loss": 1.9121,
      "step": 450
    },
    {
      "epoch": 0.12173740447914254,
      "grad_norm": 0.48583269119262695,
      "learning_rate": 0.0001878539296110082,
      "loss": 1.9523,
      "step": 460
    },
    {
      "epoch": 0.12438386979390652,
      "grad_norm": 0.6429789066314697,
      "learning_rate": 0.00018758930934109554,
      "loss": 1.9751,
      "step": 470
    },
    {
      "epoch": 0.12703033510867048,
      "grad_norm": 0.5023618936538696,
      "learning_rate": 0.00018732468907118287,
      "loss": 1.9048,
      "step": 480
    },
    {
      "epoch": 0.12967680042343446,
      "grad_norm": 0.5641553401947021,
      "learning_rate": 0.00018706006880127017,
      "loss": 1.9364,
      "step": 490
    },
    {
      "epoch": 0.13232326573819841,
      "grad_norm": 0.5877901911735535,
      "learning_rate": 0.0001867954485313575,
      "loss": 1.8602,
      "step": 500
    },
    {
      "epoch": 0.1349697310529624,
      "grad_norm": 0.5806058049201965,
      "learning_rate": 0.00018653082826144483,
      "loss": 1.8942,
      "step": 510
    },
    {
      "epoch": 0.13761619636772635,
      "grad_norm": 0.6555242538452148,
      "learning_rate": 0.00018626620799153215,
      "loss": 1.9071,
      "step": 520
    },
    {
      "epoch": 0.14026266168249032,
      "grad_norm": 0.4622182548046112,
      "learning_rate": 0.00018600158772161948,
      "loss": 1.8962,
      "step": 530
    },
    {
      "epoch": 0.1429091269972543,
      "grad_norm": 0.7852776050567627,
      "learning_rate": 0.0001857369674517068,
      "loss": 1.8527,
      "step": 540
    },
    {
      "epoch": 0.14555559231201826,
      "grad_norm": 0.5626876950263977,
      "learning_rate": 0.00018547234718179414,
      "loss": 1.9442,
      "step": 550
    },
    {
      "epoch": 0.14820205762678224,
      "grad_norm": 0.5195635557174683,
      "learning_rate": 0.00018520772691188147,
      "loss": 1.9737,
      "step": 560
    },
    {
      "epoch": 0.1508485229415462,
      "grad_norm": 0.6362683773040771,
      "learning_rate": 0.0001849431066419688,
      "loss": 1.9293,
      "step": 570
    },
    {
      "epoch": 0.15349498825631017,
      "grad_norm": 0.5575569272041321,
      "learning_rate": 0.00018467848637205612,
      "loss": 1.9384,
      "step": 580
    },
    {
      "epoch": 0.15614145357107415,
      "grad_norm": 0.5993044972419739,
      "learning_rate": 0.00018441386610214345,
      "loss": 1.9569,
      "step": 590
    },
    {
      "epoch": 0.1587879188858381,
      "grad_norm": 0.5754823088645935,
      "learning_rate": 0.00018414924583223075,
      "loss": 1.94,
      "step": 600
    },
    {
      "epoch": 0.16143438420060208,
      "grad_norm": 0.7146652340888977,
      "learning_rate": 0.00018388462556231808,
      "loss": 1.9464,
      "step": 610
    },
    {
      "epoch": 0.16408084951536603,
      "grad_norm": 0.6468725800514221,
      "learning_rate": 0.0001836200052924054,
      "loss": 1.8763,
      "step": 620
    },
    {
      "epoch": 0.16672731483013,
      "grad_norm": 0.6451604962348938,
      "learning_rate": 0.00018335538502249274,
      "loss": 1.8921,
      "step": 630
    },
    {
      "epoch": 0.169373780144894,
      "grad_norm": 0.6514512300491333,
      "learning_rate": 0.00018309076475258004,
      "loss": 1.8928,
      "step": 640
    },
    {
      "epoch": 0.17202024545965794,
      "grad_norm": 0.610998809337616,
      "learning_rate": 0.00018282614448266737,
      "loss": 1.8959,
      "step": 650
    },
    {
      "epoch": 0.17466671077442192,
      "grad_norm": 0.6084425449371338,
      "learning_rate": 0.0001825615242127547,
      "loss": 2.0054,
      "step": 660
    },
    {
      "epoch": 0.17731317608918587,
      "grad_norm": 0.6508351564407349,
      "learning_rate": 0.00018229690394284202,
      "loss": 1.8754,
      "step": 670
    },
    {
      "epoch": 0.17995964140394985,
      "grad_norm": 0.7626802921295166,
      "learning_rate": 0.00018203228367292935,
      "loss": 1.9398,
      "step": 680
    },
    {
      "epoch": 0.18260610671871383,
      "grad_norm": 0.5894259810447693,
      "learning_rate": 0.00018176766340301668,
      "loss": 1.9148,
      "step": 690
    },
    {
      "epoch": 0.18525257203347778,
      "grad_norm": 1.076718807220459,
      "learning_rate": 0.000181503043133104,
      "loss": 1.9416,
      "step": 700
    },
    {
      "epoch": 0.18789903734824176,
      "grad_norm": 0.6595178246498108,
      "learning_rate": 0.00018123842286319134,
      "loss": 1.9637,
      "step": 710
    },
    {
      "epoch": 0.1905455026630057,
      "grad_norm": 0.5876649618148804,
      "learning_rate": 0.00018097380259327867,
      "loss": 1.8283,
      "step": 720
    },
    {
      "epoch": 0.1931919679777697,
      "grad_norm": 0.7954769730567932,
      "learning_rate": 0.00018070918232336597,
      "loss": 1.9216,
      "step": 730
    },
    {
      "epoch": 0.19583843329253367,
      "grad_norm": 0.7285612225532532,
      "learning_rate": 0.0001804445620534533,
      "loss": 1.9189,
      "step": 740
    },
    {
      "epoch": 0.19848489860729762,
      "grad_norm": 0.7498707175254822,
      "learning_rate": 0.00018017994178354062,
      "loss": 1.8693,
      "step": 750
    },
    {
      "epoch": 0.2011313639220616,
      "grad_norm": 0.7730883359909058,
      "learning_rate": 0.00017991532151362795,
      "loss": 1.9504,
      "step": 760
    },
    {
      "epoch": 0.20377782923682555,
      "grad_norm": 0.7898119688034058,
      "learning_rate": 0.00017965070124371528,
      "loss": 1.8775,
      "step": 770
    },
    {
      "epoch": 0.20642429455158953,
      "grad_norm": 0.6401030421257019,
      "learning_rate": 0.0001793860809738026,
      "loss": 2.003,
      "step": 780
    },
    {
      "epoch": 0.2090707598663535,
      "grad_norm": 0.5788671374320984,
      "learning_rate": 0.00017912146070388994,
      "loss": 1.8606,
      "step": 790
    },
    {
      "epoch": 0.21171722518111746,
      "grad_norm": 0.7126042246818542,
      "learning_rate": 0.00017885684043397726,
      "loss": 1.9852,
      "step": 800
    },
    {
      "epoch": 0.21436369049588144,
      "grad_norm": 0.5134244561195374,
      "learning_rate": 0.0001785922201640646,
      "loss": 1.9778,
      "step": 810
    },
    {
      "epoch": 0.2170101558106454,
      "grad_norm": 0.6145138740539551,
      "learning_rate": 0.00017832759989415192,
      "loss": 1.8402,
      "step": 820
    },
    {
      "epoch": 0.21965662112540937,
      "grad_norm": 0.6286423802375793,
      "learning_rate": 0.00017806297962423922,
      "loss": 1.8381,
      "step": 830
    },
    {
      "epoch": 0.22230308644017335,
      "grad_norm": 0.5955551266670227,
      "learning_rate": 0.00017779835935432655,
      "loss": 1.936,
      "step": 840
    },
    {
      "epoch": 0.2249495517549373,
      "grad_norm": 0.7650172710418701,
      "learning_rate": 0.00017753373908441385,
      "loss": 1.8226,
      "step": 850
    },
    {
      "epoch": 0.22759601706970128,
      "grad_norm": 0.7098402976989746,
      "learning_rate": 0.00017726911881450118,
      "loss": 1.8304,
      "step": 860
    },
    {
      "epoch": 0.23024248238446524,
      "grad_norm": 0.5342611074447632,
      "learning_rate": 0.0001770044985445885,
      "loss": 1.9426,
      "step": 870
    },
    {
      "epoch": 0.23288894769922922,
      "grad_norm": 0.6547587513923645,
      "learning_rate": 0.00017673987827467584,
      "loss": 1.8037,
      "step": 880
    },
    {
      "epoch": 0.2355354130139932,
      "grad_norm": 0.6705029010772705,
      "learning_rate": 0.00017647525800476316,
      "loss": 1.9499,
      "step": 890
    },
    {
      "epoch": 0.23818187832875715,
      "grad_norm": 0.6064473986625671,
      "learning_rate": 0.0001762106377348505,
      "loss": 1.8114,
      "step": 900
    },
    {
      "epoch": 0.24082834364352113,
      "grad_norm": 0.653133749961853,
      "learning_rate": 0.00017594601746493782,
      "loss": 1.9138,
      "step": 910
    },
    {
      "epoch": 0.24347480895828508,
      "grad_norm": 0.701577365398407,
      "learning_rate": 0.00017568139719502515,
      "loss": 1.8855,
      "step": 920
    },
    {
      "epoch": 0.24612127427304906,
      "grad_norm": 0.693661630153656,
      "learning_rate": 0.00017541677692511248,
      "loss": 1.9237,
      "step": 930
    },
    {
      "epoch": 0.24876773958781304,
      "grad_norm": 0.6345623731613159,
      "learning_rate": 0.0001751521566551998,
      "loss": 2.0285,
      "step": 940
    },
    {
      "epoch": 0.251414204902577,
      "grad_norm": 0.607571542263031,
      "learning_rate": 0.00017488753638528713,
      "loss": 1.8159,
      "step": 950
    },
    {
      "epoch": 0.25406067021734097,
      "grad_norm": 0.5411767959594727,
      "learning_rate": 0.00017462291611537443,
      "loss": 1.8956,
      "step": 960
    },
    {
      "epoch": 0.25670713553210495,
      "grad_norm": 0.6156144142150879,
      "learning_rate": 0.00017435829584546176,
      "loss": 1.8817,
      "step": 970
    },
    {
      "epoch": 0.2593536008468689,
      "grad_norm": 0.5822585225105286,
      "learning_rate": 0.0001740936755755491,
      "loss": 1.8617,
      "step": 980
    },
    {
      "epoch": 0.26200006616163285,
      "grad_norm": 0.7051244378089905,
      "learning_rate": 0.00017382905530563642,
      "loss": 1.8263,
      "step": 990
    },
    {
      "epoch": 0.26464653147639683,
      "grad_norm": 0.7814620733261108,
      "learning_rate": 0.00017356443503572375,
      "loss": 1.7824,
      "step": 1000
    },
    {
      "epoch": 0.2672929967911608,
      "grad_norm": 0.6536579132080078,
      "learning_rate": 0.00017329981476581108,
      "loss": 1.7474,
      "step": 1010
    },
    {
      "epoch": 0.2699394621059248,
      "grad_norm": 0.6720018982887268,
      "learning_rate": 0.0001730351944958984,
      "loss": 1.7756,
      "step": 1020
    },
    {
      "epoch": 0.27258592742068877,
      "grad_norm": 0.6991193890571594,
      "learning_rate": 0.00017277057422598573,
      "loss": 1.7817,
      "step": 1030
    },
    {
      "epoch": 0.2752323927354527,
      "grad_norm": 0.6557636260986328,
      "learning_rate": 0.00017250595395607306,
      "loss": 1.839,
      "step": 1040
    },
    {
      "epoch": 0.27787885805021667,
      "grad_norm": 0.6469992995262146,
      "learning_rate": 0.0001722413336861604,
      "loss": 1.8064,
      "step": 1050
    },
    {
      "epoch": 0.28052532336498065,
      "grad_norm": 0.6776157021522522,
      "learning_rate": 0.00017197671341624772,
      "loss": 1.8124,
      "step": 1060
    },
    {
      "epoch": 0.28317178867974463,
      "grad_norm": 0.7012218832969666,
      "learning_rate": 0.00017171209314633502,
      "loss": 1.814,
      "step": 1070
    },
    {
      "epoch": 0.2858182539945086,
      "grad_norm": 0.8521841764450073,
      "learning_rate": 0.00017144747287642235,
      "loss": 1.8879,
      "step": 1080
    },
    {
      "epoch": 0.28846471930927253,
      "grad_norm": 0.6098154187202454,
      "learning_rate": 0.00017118285260650965,
      "loss": 1.9165,
      "step": 1090
    },
    {
      "epoch": 0.2911111846240365,
      "grad_norm": 0.6497148275375366,
      "learning_rate": 0.00017091823233659698,
      "loss": 1.8897,
      "step": 1100
    },
    {
      "epoch": 0.2937576499388005,
      "grad_norm": 0.8043696880340576,
      "learning_rate": 0.0001706536120666843,
      "loss": 1.8986,
      "step": 1110
    },
    {
      "epoch": 0.29640411525356447,
      "grad_norm": 0.6692444086074829,
      "learning_rate": 0.00017038899179677163,
      "loss": 1.8737,
      "step": 1120
    },
    {
      "epoch": 0.29905058056832845,
      "grad_norm": 0.6281316876411438,
      "learning_rate": 0.00017012437152685896,
      "loss": 1.8961,
      "step": 1130
    },
    {
      "epoch": 0.3016970458830924,
      "grad_norm": 0.7801333069801331,
      "learning_rate": 0.0001698597512569463,
      "loss": 1.8311,
      "step": 1140
    },
    {
      "epoch": 0.30434351119785635,
      "grad_norm": 0.6128683686256409,
      "learning_rate": 0.00016959513098703362,
      "loss": 1.8296,
      "step": 1150
    },
    {
      "epoch": 0.30698997651262033,
      "grad_norm": 0.7065849304199219,
      "learning_rate": 0.00016933051071712094,
      "loss": 1.8025,
      "step": 1160
    },
    {
      "epoch": 0.3096364418273843,
      "grad_norm": 0.6774923801422119,
      "learning_rate": 0.00016906589044720827,
      "loss": 1.8624,
      "step": 1170
    },
    {
      "epoch": 0.3122829071421483,
      "grad_norm": 0.8611317873001099,
      "learning_rate": 0.0001688012701772956,
      "loss": 1.7367,
      "step": 1180
    },
    {
      "epoch": 0.3149293724569122,
      "grad_norm": 0.7302024364471436,
      "learning_rate": 0.0001685366499073829,
      "loss": 1.8312,
      "step": 1190
    },
    {
      "epoch": 0.3175758377716762,
      "grad_norm": 0.6717813611030579,
      "learning_rate": 0.00016827202963747023,
      "loss": 1.9238,
      "step": 1200
    },
    {
      "epoch": 0.3202223030864402,
      "grad_norm": 0.7848470211029053,
      "learning_rate": 0.00016800740936755756,
      "loss": 1.8434,
      "step": 1210
    },
    {
      "epoch": 0.32286876840120415,
      "grad_norm": 0.6791436076164246,
      "learning_rate": 0.0001677427890976449,
      "loss": 1.8372,
      "step": 1220
    },
    {
      "epoch": 0.32551523371596813,
      "grad_norm": 0.7363408207893372,
      "learning_rate": 0.00016747816882773221,
      "loss": 1.7845,
      "step": 1230
    },
    {
      "epoch": 0.32816169903073206,
      "grad_norm": 0.6375470161437988,
      "learning_rate": 0.00016721354855781954,
      "loss": 1.8904,
      "step": 1240
    },
    {
      "epoch": 0.33080816434549604,
      "grad_norm": 0.689701497554779,
      "learning_rate": 0.00016694892828790687,
      "loss": 1.7681,
      "step": 1250
    },
    {
      "epoch": 0.33345462966026,
      "grad_norm": 0.676527738571167,
      "learning_rate": 0.0001666843080179942,
      "loss": 1.8869,
      "step": 1260
    },
    {
      "epoch": 0.336101094975024,
      "grad_norm": 0.491085946559906,
      "learning_rate": 0.00016641968774808153,
      "loss": 1.8283,
      "step": 1270
    },
    {
      "epoch": 0.338747560289788,
      "grad_norm": 0.5792438387870789,
      "learning_rate": 0.00016615506747816886,
      "loss": 1.8883,
      "step": 1280
    },
    {
      "epoch": 0.3413940256045519,
      "grad_norm": 0.6925298571586609,
      "learning_rate": 0.00016589044720825616,
      "loss": 1.8697,
      "step": 1290
    },
    {
      "epoch": 0.3440404909193159,
      "grad_norm": 0.6610876321792603,
      "learning_rate": 0.00016562582693834349,
      "loss": 1.8436,
      "step": 1300
    },
    {
      "epoch": 0.34668695623407986,
      "grad_norm": 0.6787265539169312,
      "learning_rate": 0.00016536120666843081,
      "loss": 1.8475,
      "step": 1310
    },
    {
      "epoch": 0.34933342154884384,
      "grad_norm": 0.5828381776809692,
      "learning_rate": 0.00016509658639851811,
      "loss": 1.8311,
      "step": 1320
    },
    {
      "epoch": 0.3519798868636078,
      "grad_norm": 0.7118292450904846,
      "learning_rate": 0.00016483196612860544,
      "loss": 1.9474,
      "step": 1330
    },
    {
      "epoch": 0.35462635217837174,
      "grad_norm": 0.854207456111908,
      "learning_rate": 0.00016456734585869277,
      "loss": 1.8184,
      "step": 1340
    },
    {
      "epoch": 0.3572728174931357,
      "grad_norm": 0.7115711569786072,
      "learning_rate": 0.0001643027255887801,
      "loss": 1.8965,
      "step": 1350
    },
    {
      "epoch": 0.3599192828078997,
      "grad_norm": 0.6583244800567627,
      "learning_rate": 0.00016403810531886743,
      "loss": 1.8204,
      "step": 1360
    },
    {
      "epoch": 0.3625657481226637,
      "grad_norm": 0.79815274477005,
      "learning_rate": 0.00016377348504895476,
      "loss": 1.9433,
      "step": 1370
    },
    {
      "epoch": 0.36521221343742766,
      "grad_norm": 0.8672485947608948,
      "learning_rate": 0.00016350886477904208,
      "loss": 1.9096,
      "step": 1380
    },
    {
      "epoch": 0.3678586787521916,
      "grad_norm": 0.6584401726722717,
      "learning_rate": 0.0001632442445091294,
      "loss": 1.8926,
      "step": 1390
    },
    {
      "epoch": 0.37050514406695556,
      "grad_norm": 0.6587731242179871,
      "learning_rate": 0.00016297962423921674,
      "loss": 1.8409,
      "step": 1400
    },
    {
      "epoch": 0.37315160938171954,
      "grad_norm": 0.7461519241333008,
      "learning_rate": 0.00016271500396930407,
      "loss": 1.9103,
      "step": 1410
    },
    {
      "epoch": 0.3757980746964835,
      "grad_norm": 0.6436218023300171,
      "learning_rate": 0.0001624503836993914,
      "loss": 1.7785,
      "step": 1420
    },
    {
      "epoch": 0.3784445400112475,
      "grad_norm": 0.6497849225997925,
      "learning_rate": 0.0001621857634294787,
      "loss": 1.7646,
      "step": 1430
    },
    {
      "epoch": 0.3810910053260114,
      "grad_norm": 0.6989878416061401,
      "learning_rate": 0.00016192114315956603,
      "loss": 1.8875,
      "step": 1440
    },
    {
      "epoch": 0.3837374706407754,
      "grad_norm": 0.9750683903694153,
      "learning_rate": 0.00016165652288965335,
      "loss": 1.8165,
      "step": 1450
    },
    {
      "epoch": 0.3863839359555394,
      "grad_norm": 0.7101098299026489,
      "learning_rate": 0.00016139190261974068,
      "loss": 1.9091,
      "step": 1460
    },
    {
      "epoch": 0.38903040127030336,
      "grad_norm": 0.65897536277771,
      "learning_rate": 0.000161127282349828,
      "loss": 1.825,
      "step": 1470
    },
    {
      "epoch": 0.39167686658506734,
      "grad_norm": 0.6187789440155029,
      "learning_rate": 0.00016086266207991534,
      "loss": 1.7695,
      "step": 1480
    },
    {
      "epoch": 0.39432333189983126,
      "grad_norm": 0.8342849016189575,
      "learning_rate": 0.00016059804181000267,
      "loss": 1.8351,
      "step": 1490
    },
    {
      "epoch": 0.39696979721459524,
      "grad_norm": 0.7752073407173157,
      "learning_rate": 0.00016033342154008997,
      "loss": 1.8758,
      "step": 1500
    },
    {
      "epoch": 0.3996162625293592,
      "grad_norm": 0.6384215950965881,
      "learning_rate": 0.0001600688012701773,
      "loss": 1.894,
      "step": 1510
    },
    {
      "epoch": 0.4022627278441232,
      "grad_norm": 0.6276379227638245,
      "learning_rate": 0.00015980418100026463,
      "loss": 1.9009,
      "step": 1520
    },
    {
      "epoch": 0.4049091931588872,
      "grad_norm": 0.7169933319091797,
      "learning_rate": 0.00015953956073035195,
      "loss": 1.8128,
      "step": 1530
    },
    {
      "epoch": 0.4075556584736511,
      "grad_norm": 0.7674745917320251,
      "learning_rate": 0.00015927494046043928,
      "loss": 1.8414,
      "step": 1540
    },
    {
      "epoch": 0.4102021237884151,
      "grad_norm": 0.859555721282959,
      "learning_rate": 0.00015901032019052658,
      "loss": 1.8609,
      "step": 1550
    },
    {
      "epoch": 0.41284858910317906,
      "grad_norm": 0.6905356049537659,
      "learning_rate": 0.0001587456999206139,
      "loss": 1.9199,
      "step": 1560
    },
    {
      "epoch": 0.41549505441794304,
      "grad_norm": 0.6172357797622681,
      "learning_rate": 0.00015848107965070124,
      "loss": 1.732,
      "step": 1570
    },
    {
      "epoch": 0.418141519732707,
      "grad_norm": 0.5673511028289795,
      "learning_rate": 0.00015821645938078857,
      "loss": 1.8401,
      "step": 1580
    },
    {
      "epoch": 0.42078798504747095,
      "grad_norm": 0.7461394667625427,
      "learning_rate": 0.0001579518391108759,
      "loss": 1.8566,
      "step": 1590
    },
    {
      "epoch": 0.4234344503622349,
      "grad_norm": 0.5818058848381042,
      "learning_rate": 0.00015768721884096322,
      "loss": 1.9523,
      "step": 1600
    },
    {
      "epoch": 0.4260809156769989,
      "grad_norm": 0.7412375211715698,
      "learning_rate": 0.00015742259857105055,
      "loss": 1.8737,
      "step": 1610
    },
    {
      "epoch": 0.4287273809917629,
      "grad_norm": 0.6475711464881897,
      "learning_rate": 0.00015715797830113788,
      "loss": 1.9035,
      "step": 1620
    },
    {
      "epoch": 0.43137384630652686,
      "grad_norm": 0.7588614225387573,
      "learning_rate": 0.0001568933580312252,
      "loss": 1.8759,
      "step": 1630
    },
    {
      "epoch": 0.4340203116212908,
      "grad_norm": 0.6595726013183594,
      "learning_rate": 0.00015662873776131254,
      "loss": 1.8075,
      "step": 1640
    },
    {
      "epoch": 0.43666677693605477,
      "grad_norm": 0.8320454359054565,
      "learning_rate": 0.00015636411749139986,
      "loss": 1.892,
      "step": 1650
    },
    {
      "epoch": 0.43931324225081875,
      "grad_norm": 0.8755031824111938,
      "learning_rate": 0.00015609949722148717,
      "loss": 1.883,
      "step": 1660
    },
    {
      "epoch": 0.4419597075655827,
      "grad_norm": 0.6586777567863464,
      "learning_rate": 0.0001558348769515745,
      "loss": 1.8455,
      "step": 1670
    },
    {
      "epoch": 0.4446061728803467,
      "grad_norm": 0.9123456478118896,
      "learning_rate": 0.00015557025668166182,
      "loss": 1.8697,
      "step": 1680
    },
    {
      "epoch": 0.44725263819511063,
      "grad_norm": 0.7417462468147278,
      "learning_rate": 0.00015530563641174915,
      "loss": 1.7193,
      "step": 1690
    },
    {
      "epoch": 0.4498991035098746,
      "grad_norm": 0.7045815587043762,
      "learning_rate": 0.00015504101614183648,
      "loss": 1.6943,
      "step": 1700
    },
    {
      "epoch": 0.4525455688246386,
      "grad_norm": 0.7162864804267883,
      "learning_rate": 0.0001547763958719238,
      "loss": 1.7545,
      "step": 1710
    },
    {
      "epoch": 0.45519203413940257,
      "grad_norm": 0.9742207527160645,
      "learning_rate": 0.0001545117756020111,
      "loss": 1.862,
      "step": 1720
    },
    {
      "epoch": 0.45783849945416655,
      "grad_norm": 0.7088436484336853,
      "learning_rate": 0.00015424715533209844,
      "loss": 1.9346,
      "step": 1730
    },
    {
      "epoch": 0.46048496476893047,
      "grad_norm": 0.6833820939064026,
      "learning_rate": 0.00015398253506218576,
      "loss": 1.8071,
      "step": 1740
    },
    {
      "epoch": 0.46313143008369445,
      "grad_norm": 0.6904230713844299,
      "learning_rate": 0.0001537179147922731,
      "loss": 1.8638,
      "step": 1750
    },
    {
      "epoch": 0.46577789539845843,
      "grad_norm": 0.6724717617034912,
      "learning_rate": 0.00015345329452236042,
      "loss": 1.8926,
      "step": 1760
    },
    {
      "epoch": 0.4684243607132224,
      "grad_norm": 0.8159484267234802,
      "learning_rate": 0.00015318867425244775,
      "loss": 1.8194,
      "step": 1770
    },
    {
      "epoch": 0.4710708260279864,
      "grad_norm": 0.738295316696167,
      "learning_rate": 0.00015292405398253508,
      "loss": 1.7572,
      "step": 1780
    },
    {
      "epoch": 0.4737172913427503,
      "grad_norm": 0.8079731464385986,
      "learning_rate": 0.00015265943371262238,
      "loss": 1.7817,
      "step": 1790
    },
    {
      "epoch": 0.4763637566575143,
      "grad_norm": 0.6169954538345337,
      "learning_rate": 0.0001523948134427097,
      "loss": 1.7598,
      "step": 1800
    },
    {
      "epoch": 0.47901022197227827,
      "grad_norm": 0.7137024402618408,
      "learning_rate": 0.00015213019317279704,
      "loss": 1.7665,
      "step": 1810
    },
    {
      "epoch": 0.48165668728704225,
      "grad_norm": 0.7165957093238831,
      "learning_rate": 0.00015186557290288436,
      "loss": 1.8279,
      "step": 1820
    },
    {
      "epoch": 0.48430315260180623,
      "grad_norm": 0.7564166188240051,
      "learning_rate": 0.0001516009526329717,
      "loss": 1.8017,
      "step": 1830
    },
    {
      "epoch": 0.48694961791657015,
      "grad_norm": 0.8608114719390869,
      "learning_rate": 0.00015133633236305902,
      "loss": 1.8795,
      "step": 1840
    },
    {
      "epoch": 0.48959608323133413,
      "grad_norm": 0.7714924812316895,
      "learning_rate": 0.00015107171209314635,
      "loss": 1.8818,
      "step": 1850
    },
    {
      "epoch": 0.4922425485460981,
      "grad_norm": 0.8169373869895935,
      "learning_rate": 0.00015080709182323368,
      "loss": 1.8752,
      "step": 1860
    },
    {
      "epoch": 0.4948890138608621,
      "grad_norm": 0.9300076961517334,
      "learning_rate": 0.000150542471553321,
      "loss": 1.8853,
      "step": 1870
    },
    {
      "epoch": 0.49753547917562607,
      "grad_norm": 0.683039665222168,
      "learning_rate": 0.00015027785128340833,
      "loss": 1.8472,
      "step": 1880
    },
    {
      "epoch": 0.50018194449039,
      "grad_norm": 0.7152121663093567,
      "learning_rate": 0.00015001323101349566,
      "loss": 1.8653,
      "step": 1890
    },
    {
      "epoch": 0.502828409805154,
      "grad_norm": 0.625318706035614,
      "learning_rate": 0.00014974861074358296,
      "loss": 1.7294,
      "step": 1900
    },
    {
      "epoch": 0.505474875119918,
      "grad_norm": 0.611918032169342,
      "learning_rate": 0.0001494839904736703,
      "loss": 1.8382,
      "step": 1910
    },
    {
      "epoch": 0.5081213404346819,
      "grad_norm": 0.6957367658615112,
      "learning_rate": 0.00014921937020375762,
      "loss": 1.7649,
      "step": 1920
    },
    {
      "epoch": 0.5107678057494459,
      "grad_norm": 0.6162023544311523,
      "learning_rate": 0.00014895474993384495,
      "loss": 1.8368,
      "step": 1930
    },
    {
      "epoch": 0.5134142710642099,
      "grad_norm": 0.6098549365997314,
      "learning_rate": 0.00014869012966393225,
      "loss": 1.8371,
      "step": 1940
    },
    {
      "epoch": 0.5160607363789739,
      "grad_norm": 0.9935314059257507,
      "learning_rate": 0.00014842550939401958,
      "loss": 1.8127,
      "step": 1950
    },
    {
      "epoch": 0.5187072016937379,
      "grad_norm": 0.7571781873703003,
      "learning_rate": 0.0001481608891241069,
      "loss": 1.7846,
      "step": 1960
    },
    {
      "epoch": 0.5213536670085017,
      "grad_norm": 0.8474048376083374,
      "learning_rate": 0.00014789626885419423,
      "loss": 1.8242,
      "step": 1970
    },
    {
      "epoch": 0.5240001323232657,
      "grad_norm": 0.7100618481636047,
      "learning_rate": 0.00014763164858428156,
      "loss": 1.8554,
      "step": 1980
    },
    {
      "epoch": 0.5266465976380297,
      "grad_norm": 0.556943953037262,
      "learning_rate": 0.0001473670283143689,
      "loss": 1.8228,
      "step": 1990
    },
    {
      "epoch": 0.5292930629527937,
      "grad_norm": 0.6181616187095642,
      "learning_rate": 0.00014710240804445622,
      "loss": 1.8642,
      "step": 2000
    },
    {
      "epoch": 0.5319395282675576,
      "grad_norm": 0.7117794752120972,
      "learning_rate": 0.00014683778777454355,
      "loss": 1.8494,
      "step": 2010
    },
    {
      "epoch": 0.5345859935823216,
      "grad_norm": 0.7652099132537842,
      "learning_rate": 0.00014657316750463085,
      "loss": 1.7646,
      "step": 2020
    },
    {
      "epoch": 0.5372324588970856,
      "grad_norm": 0.6969843506813049,
      "learning_rate": 0.00014630854723471817,
      "loss": 1.7481,
      "step": 2030
    },
    {
      "epoch": 0.5398789242118496,
      "grad_norm": 0.6179986000061035,
      "learning_rate": 0.0001460439269648055,
      "loss": 1.7625,
      "step": 2040
    },
    {
      "epoch": 0.5425253895266136,
      "grad_norm": 0.8537712693214417,
      "learning_rate": 0.00014577930669489283,
      "loss": 1.7812,
      "step": 2050
    },
    {
      "epoch": 0.5451718548413775,
      "grad_norm": 0.6187912821769714,
      "learning_rate": 0.00014551468642498016,
      "loss": 1.7424,
      "step": 2060
    },
    {
      "epoch": 0.5478183201561414,
      "grad_norm": 0.7447651028633118,
      "learning_rate": 0.0001452500661550675,
      "loss": 1.842,
      "step": 2070
    },
    {
      "epoch": 0.5504647854709054,
      "grad_norm": 0.6364145278930664,
      "learning_rate": 0.00014498544588515482,
      "loss": 1.7836,
      "step": 2080
    },
    {
      "epoch": 0.5531112507856694,
      "grad_norm": 0.6441702842712402,
      "learning_rate": 0.00014472082561524214,
      "loss": 1.8112,
      "step": 2090
    },
    {
      "epoch": 0.5557577161004333,
      "grad_norm": 0.7075929641723633,
      "learning_rate": 0.00014445620534532947,
      "loss": 1.8289,
      "step": 2100
    },
    {
      "epoch": 0.5584041814151973,
      "grad_norm": 0.8167605400085449,
      "learning_rate": 0.0001441915850754168,
      "loss": 1.8316,
      "step": 2110
    },
    {
      "epoch": 0.5610506467299613,
      "grad_norm": 0.7554447054862976,
      "learning_rate": 0.00014392696480550413,
      "loss": 1.7832,
      "step": 2120
    },
    {
      "epoch": 0.5636971120447253,
      "grad_norm": 0.6595631241798401,
      "learning_rate": 0.00014366234453559143,
      "loss": 1.7732,
      "step": 2130
    },
    {
      "epoch": 0.5663435773594893,
      "grad_norm": 0.8315140008926392,
      "learning_rate": 0.00014339772426567876,
      "loss": 1.7161,
      "step": 2140
    },
    {
      "epoch": 0.5689900426742532,
      "grad_norm": 0.7984774112701416,
      "learning_rate": 0.0001431331039957661,
      "loss": 1.7621,
      "step": 2150
    },
    {
      "epoch": 0.5716365079890172,
      "grad_norm": 0.6514248251914978,
      "learning_rate": 0.0001428684837258534,
      "loss": 1.8076,
      "step": 2160
    },
    {
      "epoch": 0.5742829733037811,
      "grad_norm": 0.7683327794075012,
      "learning_rate": 0.00014260386345594072,
      "loss": 1.6896,
      "step": 2170
    },
    {
      "epoch": 0.5769294386185451,
      "grad_norm": 0.7002519965171814,
      "learning_rate": 0.00014233924318602804,
      "loss": 1.8435,
      "step": 2180
    },
    {
      "epoch": 0.579575903933309,
      "grad_norm": 0.6496331095695496,
      "learning_rate": 0.00014207462291611537,
      "loss": 1.7242,
      "step": 2190
    },
    {
      "epoch": 0.582222369248073,
      "grad_norm": 0.7975449562072754,
      "learning_rate": 0.0001418100026462027,
      "loss": 1.8502,
      "step": 2200
    },
    {
      "epoch": 0.584868834562837,
      "grad_norm": 0.7065280675888062,
      "learning_rate": 0.00014154538237629003,
      "loss": 1.8478,
      "step": 2210
    },
    {
      "epoch": 0.587515299877601,
      "grad_norm": 0.7360895276069641,
      "learning_rate": 0.00014128076210637736,
      "loss": 1.8698,
      "step": 2220
    },
    {
      "epoch": 0.590161765192365,
      "grad_norm": 0.7786563038825989,
      "learning_rate": 0.00014101614183646469,
      "loss": 1.8301,
      "step": 2230
    },
    {
      "epoch": 0.5928082305071289,
      "grad_norm": 0.7517759203910828,
      "learning_rate": 0.000140751521566552,
      "loss": 1.8399,
      "step": 2240
    },
    {
      "epoch": 0.5954546958218929,
      "grad_norm": 0.8484557867050171,
      "learning_rate": 0.00014048690129663934,
      "loss": 1.9068,
      "step": 2250
    },
    {
      "epoch": 0.5981011611366569,
      "grad_norm": 0.7885953187942505,
      "learning_rate": 0.00014022228102672664,
      "loss": 1.7317,
      "step": 2260
    },
    {
      "epoch": 0.6007476264514208,
      "grad_norm": 0.5986354947090149,
      "learning_rate": 0.00013995766075681397,
      "loss": 1.7952,
      "step": 2270
    },
    {
      "epoch": 0.6033940917661847,
      "grad_norm": 0.6745650768280029,
      "learning_rate": 0.0001396930404869013,
      "loss": 1.7712,
      "step": 2280
    },
    {
      "epoch": 0.6060405570809487,
      "grad_norm": 0.6114712953567505,
      "learning_rate": 0.00013942842021698863,
      "loss": 1.7962,
      "step": 2290
    },
    {
      "epoch": 0.6086870223957127,
      "grad_norm": 0.6380239129066467,
      "learning_rate": 0.00013916379994707596,
      "loss": 1.844,
      "step": 2300
    },
    {
      "epoch": 0.6113334877104767,
      "grad_norm": 0.7643340826034546,
      "learning_rate": 0.00013889917967716328,
      "loss": 1.8071,
      "step": 2310
    },
    {
      "epoch": 0.6139799530252407,
      "grad_norm": 0.577856183052063,
      "learning_rate": 0.0001386345594072506,
      "loss": 1.9114,
      "step": 2320
    },
    {
      "epoch": 0.6166264183400046,
      "grad_norm": 0.9383927583694458,
      "learning_rate": 0.00013836993913733794,
      "loss": 1.9195,
      "step": 2330
    },
    {
      "epoch": 0.6192728836547686,
      "grad_norm": 0.6564637422561646,
      "learning_rate": 0.00013810531886742527,
      "loss": 1.8571,
      "step": 2340
    },
    {
      "epoch": 0.6219193489695326,
      "grad_norm": 0.7302878499031067,
      "learning_rate": 0.0001378406985975126,
      "loss": 1.7018,
      "step": 2350
    },
    {
      "epoch": 0.6245658142842966,
      "grad_norm": 0.7207677960395813,
      "learning_rate": 0.0001375760783275999,
      "loss": 1.7917,
      "step": 2360
    },
    {
      "epoch": 0.6272122795990605,
      "grad_norm": 0.7587428092956543,
      "learning_rate": 0.00013731145805768723,
      "loss": 1.7184,
      "step": 2370
    },
    {
      "epoch": 0.6298587449138244,
      "grad_norm": 0.6722824573516846,
      "learning_rate": 0.00013704683778777453,
      "loss": 1.8007,
      "step": 2380
    },
    {
      "epoch": 0.6325052102285884,
      "grad_norm": 0.7505239844322205,
      "learning_rate": 0.00013678221751786186,
      "loss": 1.794,
      "step": 2390
    },
    {
      "epoch": 0.6351516755433524,
      "grad_norm": 0.8643547892570496,
      "learning_rate": 0.00013651759724794918,
      "loss": 1.7996,
      "step": 2400
    },
    {
      "epoch": 0.6377981408581164,
      "grad_norm": 0.6545624136924744,
      "learning_rate": 0.0001362529769780365,
      "loss": 1.8259,
      "step": 2410
    },
    {
      "epoch": 0.6404446061728803,
      "grad_norm": 0.7927228212356567,
      "learning_rate": 0.00013598835670812384,
      "loss": 1.7597,
      "step": 2420
    },
    {
      "epoch": 0.6430910714876443,
      "grad_norm": 0.7167884707450867,
      "learning_rate": 0.00013572373643821117,
      "loss": 1.8415,
      "step": 2430
    },
    {
      "epoch": 0.6457375368024083,
      "grad_norm": 0.728705883026123,
      "learning_rate": 0.0001354591161682985,
      "loss": 1.7206,
      "step": 2440
    },
    {
      "epoch": 0.6483840021171723,
      "grad_norm": 0.8025102019309998,
      "learning_rate": 0.00013519449589838582,
      "loss": 1.8775,
      "step": 2450
    },
    {
      "epoch": 0.6510304674319363,
      "grad_norm": 0.8029478788375854,
      "learning_rate": 0.00013492987562847315,
      "loss": 1.7276,
      "step": 2460
    },
    {
      "epoch": 0.6536769327467001,
      "grad_norm": 0.7091700434684753,
      "learning_rate": 0.00013466525535856048,
      "loss": 1.767,
      "step": 2470
    },
    {
      "epoch": 0.6563233980614641,
      "grad_norm": 0.6862803101539612,
      "learning_rate": 0.0001344006350886478,
      "loss": 1.6993,
      "step": 2480
    },
    {
      "epoch": 0.6589698633762281,
      "grad_norm": 0.8512476682662964,
      "learning_rate": 0.0001341360148187351,
      "loss": 1.7763,
      "step": 2490
    },
    {
      "epoch": 0.6616163286909921,
      "grad_norm": 0.5597408413887024,
      "learning_rate": 0.00013387139454882244,
      "loss": 1.8255,
      "step": 2500
    },
    {
      "epoch": 0.664262794005756,
      "grad_norm": 0.7199106812477112,
      "learning_rate": 0.00013360677427890977,
      "loss": 1.8067,
      "step": 2510
    },
    {
      "epoch": 0.66690925932052,
      "grad_norm": 0.6813410520553589,
      "learning_rate": 0.0001333421540089971,
      "loss": 1.7748,
      "step": 2520
    },
    {
      "epoch": 0.669555724635284,
      "grad_norm": 0.6534729599952698,
      "learning_rate": 0.00013307753373908442,
      "loss": 1.7344,
      "step": 2530
    },
    {
      "epoch": 0.672202189950048,
      "grad_norm": 0.6619261503219604,
      "learning_rate": 0.00013281291346917175,
      "loss": 1.773,
      "step": 2540
    },
    {
      "epoch": 0.674848655264812,
      "grad_norm": 0.6684876680374146,
      "learning_rate": 0.00013254829319925908,
      "loss": 1.7784,
      "step": 2550
    },
    {
      "epoch": 0.677495120579576,
      "grad_norm": 0.8412331342697144,
      "learning_rate": 0.0001322836729293464,
      "loss": 1.7458,
      "step": 2560
    },
    {
      "epoch": 0.6801415858943398,
      "grad_norm": 0.7343162298202515,
      "learning_rate": 0.00013201905265943374,
      "loss": 1.8433,
      "step": 2570
    },
    {
      "epoch": 0.6827880512091038,
      "grad_norm": 0.7613471746444702,
      "learning_rate": 0.00013175443238952106,
      "loss": 1.6776,
      "step": 2580
    },
    {
      "epoch": 0.6854345165238678,
      "grad_norm": 0.8165135979652405,
      "learning_rate": 0.00013148981211960837,
      "loss": 1.8019,
      "step": 2590
    },
    {
      "epoch": 0.6880809818386318,
      "grad_norm": 0.8203070759773254,
      "learning_rate": 0.0001312251918496957,
      "loss": 1.8176,
      "step": 2600
    },
    {
      "epoch": 0.6907274471533957,
      "grad_norm": 0.7770431637763977,
      "learning_rate": 0.00013096057157978302,
      "loss": 1.7215,
      "step": 2610
    },
    {
      "epoch": 0.6933739124681597,
      "grad_norm": 0.8382050395011902,
      "learning_rate": 0.00013069595130987032,
      "loss": 1.8198,
      "step": 2620
    },
    {
      "epoch": 0.6960203777829237,
      "grad_norm": 0.71565842628479,
      "learning_rate": 0.00013043133103995765,
      "loss": 1.8644,
      "step": 2630
    },
    {
      "epoch": 0.6986668430976877,
      "grad_norm": 0.7688611149787903,
      "learning_rate": 0.00013016671077004498,
      "loss": 1.8424,
      "step": 2640
    },
    {
      "epoch": 0.7013133084124517,
      "grad_norm": 0.6644366383552551,
      "learning_rate": 0.0001299020905001323,
      "loss": 1.7529,
      "step": 2650
    },
    {
      "epoch": 0.7039597737272156,
      "grad_norm": 0.9065699577331543,
      "learning_rate": 0.00012963747023021964,
      "loss": 1.8768,
      "step": 2660
    },
    {
      "epoch": 0.7066062390419796,
      "grad_norm": 0.7844873070716858,
      "learning_rate": 0.00012937284996030696,
      "loss": 1.6939,
      "step": 2670
    },
    {
      "epoch": 0.7092527043567435,
      "grad_norm": 0.7500993609428406,
      "learning_rate": 0.0001291082296903943,
      "loss": 1.744,
      "step": 2680
    },
    {
      "epoch": 0.7118991696715075,
      "grad_norm": 0.7799549698829651,
      "learning_rate": 0.00012884360942048162,
      "loss": 1.8928,
      "step": 2690
    },
    {
      "epoch": 0.7145456349862714,
      "grad_norm": 0.7296386361122131,
      "learning_rate": 0.00012857898915056895,
      "loss": 1.87,
      "step": 2700
    },
    {
      "epoch": 0.7171921003010354,
      "grad_norm": 0.6742758750915527,
      "learning_rate": 0.00012831436888065628,
      "loss": 1.6913,
      "step": 2710
    },
    {
      "epoch": 0.7198385656157994,
      "grad_norm": 0.7626588344573975,
      "learning_rate": 0.00012804974861074358,
      "loss": 1.8371,
      "step": 2720
    },
    {
      "epoch": 0.7224850309305634,
      "grad_norm": 0.8552152514457703,
      "learning_rate": 0.0001277851283408309,
      "loss": 1.7273,
      "step": 2730
    },
    {
      "epoch": 0.7251314962453274,
      "grad_norm": 0.7406478524208069,
      "learning_rate": 0.00012752050807091824,
      "loss": 1.8533,
      "step": 2740
    },
    {
      "epoch": 0.7277779615600913,
      "grad_norm": 0.9364367723464966,
      "learning_rate": 0.00012725588780100556,
      "loss": 1.8642,
      "step": 2750
    },
    {
      "epoch": 0.7304244268748553,
      "grad_norm": 0.6678916215896606,
      "learning_rate": 0.0001269912675310929,
      "loss": 1.8382,
      "step": 2760
    },
    {
      "epoch": 0.7330708921896193,
      "grad_norm": 0.6907769441604614,
      "learning_rate": 0.00012672664726118022,
      "loss": 1.8053,
      "step": 2770
    },
    {
      "epoch": 0.7357173575043832,
      "grad_norm": 0.7243987917900085,
      "learning_rate": 0.00012646202699126755,
      "loss": 1.848,
      "step": 2780
    },
    {
      "epoch": 0.7383638228191471,
      "grad_norm": 0.6839269995689392,
      "learning_rate": 0.00012619740672135488,
      "loss": 1.875,
      "step": 2790
    },
    {
      "epoch": 0.7410102881339111,
      "grad_norm": 0.5863540172576904,
      "learning_rate": 0.0001259327864514422,
      "loss": 1.8146,
      "step": 2800
    },
    {
      "epoch": 0.7436567534486751,
      "grad_norm": 0.707221269607544,
      "learning_rate": 0.0001256681661815295,
      "loss": 1.8152,
      "step": 2810
    },
    {
      "epoch": 0.7463032187634391,
      "grad_norm": 0.7957317233085632,
      "learning_rate": 0.00012540354591161683,
      "loss": 1.7886,
      "step": 2820
    },
    {
      "epoch": 0.7489496840782031,
      "grad_norm": 0.5924646854400635,
      "learning_rate": 0.00012513892564170416,
      "loss": 1.7898,
      "step": 2830
    },
    {
      "epoch": 0.751596149392967,
      "grad_norm": 0.9116058945655823,
      "learning_rate": 0.0001248743053717915,
      "loss": 1.8497,
      "step": 2840
    },
    {
      "epoch": 0.754242614707731,
      "grad_norm": 0.8379611372947693,
      "learning_rate": 0.0001246096851018788,
      "loss": 1.837,
      "step": 2850
    },
    {
      "epoch": 0.756889080022495,
      "grad_norm": 0.8065146803855896,
      "learning_rate": 0.00012434506483196612,
      "loss": 1.8471,
      "step": 2860
    },
    {
      "epoch": 0.759535545337259,
      "grad_norm": 0.7648032307624817,
      "learning_rate": 0.00012408044456205345,
      "loss": 1.8473,
      "step": 2870
    },
    {
      "epoch": 0.7621820106520228,
      "grad_norm": 0.5755192041397095,
      "learning_rate": 0.00012381582429214078,
      "loss": 1.8237,
      "step": 2880
    },
    {
      "epoch": 0.7648284759667868,
      "grad_norm": 0.6730307340621948,
      "learning_rate": 0.0001235512040222281,
      "loss": 1.7865,
      "step": 2890
    },
    {
      "epoch": 0.7674749412815508,
      "grad_norm": 0.9253338575363159,
      "learning_rate": 0.00012328658375231543,
      "loss": 1.7987,
      "step": 2900
    },
    {
      "epoch": 0.7701214065963148,
      "grad_norm": 0.6552165150642395,
      "learning_rate": 0.00012302196348240276,
      "loss": 1.7818,
      "step": 2910
    },
    {
      "epoch": 0.7727678719110788,
      "grad_norm": 0.7323944568634033,
      "learning_rate": 0.0001227573432124901,
      "loss": 1.7616,
      "step": 2920
    },
    {
      "epoch": 0.7754143372258427,
      "grad_norm": 0.8575165271759033,
      "learning_rate": 0.00012249272294257742,
      "loss": 1.8728,
      "step": 2930
    },
    {
      "epoch": 0.7780608025406067,
      "grad_norm": 0.6787669658660889,
      "learning_rate": 0.00012222810267266475,
      "loss": 1.7126,
      "step": 2940
    },
    {
      "epoch": 0.7807072678553707,
      "grad_norm": 0.7977136969566345,
      "learning_rate": 0.00012196348240275206,
      "loss": 1.7724,
      "step": 2950
    },
    {
      "epoch": 0.7833537331701347,
      "grad_norm": 0.7814580798149109,
      "learning_rate": 0.00012169886213283939,
      "loss": 1.7416,
      "step": 2960
    },
    {
      "epoch": 0.7860001984848987,
      "grad_norm": 0.751808762550354,
      "learning_rate": 0.00012143424186292672,
      "loss": 1.7471,
      "step": 2970
    },
    {
      "epoch": 0.7886466637996625,
      "grad_norm": 0.6556122899055481,
      "learning_rate": 0.00012116962159301403,
      "loss": 1.9119,
      "step": 2980
    },
    {
      "epoch": 0.7912931291144265,
      "grad_norm": 0.7306878566741943,
      "learning_rate": 0.00012090500132310136,
      "loss": 1.8284,
      "step": 2990
    },
    {
      "epoch": 0.7939395944291905,
      "grad_norm": 0.6869848966598511,
      "learning_rate": 0.00012064038105318869,
      "loss": 1.7574,
      "step": 3000
    },
    {
      "epoch": 0.7965860597439545,
      "grad_norm": 0.6927973628044128,
      "learning_rate": 0.00012037576078327602,
      "loss": 1.7602,
      "step": 3010
    },
    {
      "epoch": 0.7992325250587184,
      "grad_norm": 0.7249860763549805,
      "learning_rate": 0.00012011114051336332,
      "loss": 1.8177,
      "step": 3020
    },
    {
      "epoch": 0.8018789903734824,
      "grad_norm": 0.6786046624183655,
      "learning_rate": 0.00011984652024345065,
      "loss": 1.7337,
      "step": 3030
    },
    {
      "epoch": 0.8045254556882464,
      "grad_norm": 0.566790759563446,
      "learning_rate": 0.00011958189997353797,
      "loss": 1.7985,
      "step": 3040
    },
    {
      "epoch": 0.8071719210030104,
      "grad_norm": 0.7225803136825562,
      "learning_rate": 0.0001193172797036253,
      "loss": 1.7579,
      "step": 3050
    },
    {
      "epoch": 0.8098183863177744,
      "grad_norm": 0.7683912515640259,
      "learning_rate": 0.00011905265943371262,
      "loss": 1.8178,
      "step": 3060
    },
    {
      "epoch": 0.8124648516325383,
      "grad_norm": 0.6689078211784363,
      "learning_rate": 0.00011878803916379994,
      "loss": 1.7657,
      "step": 3070
    },
    {
      "epoch": 0.8151113169473022,
      "grad_norm": 0.7342934012413025,
      "learning_rate": 0.00011852341889388727,
      "loss": 1.8112,
      "step": 3080
    },
    {
      "epoch": 0.8177577822620662,
      "grad_norm": 0.7027153968811035,
      "learning_rate": 0.0001182587986239746,
      "loss": 1.7694,
      "step": 3090
    },
    {
      "epoch": 0.8204042475768302,
      "grad_norm": 0.6515271067619324,
      "learning_rate": 0.00011799417835406193,
      "loss": 1.6675,
      "step": 3100
    },
    {
      "epoch": 0.8230507128915941,
      "grad_norm": 0.8789651989936829,
      "learning_rate": 0.00011772955808414924,
      "loss": 1.8008,
      "step": 3110
    },
    {
      "epoch": 0.8256971782063581,
      "grad_norm": 0.7672422528266907,
      "learning_rate": 0.00011746493781423657,
      "loss": 1.7812,
      "step": 3120
    },
    {
      "epoch": 0.8283436435211221,
      "grad_norm": 0.677629292011261,
      "learning_rate": 0.0001172003175443239,
      "loss": 1.8002,
      "step": 3130
    },
    {
      "epoch": 0.8309901088358861,
      "grad_norm": 0.6046116352081299,
      "learning_rate": 0.00011693569727441123,
      "loss": 1.7737,
      "step": 3140
    },
    {
      "epoch": 0.8336365741506501,
      "grad_norm": 0.66194087266922,
      "learning_rate": 0.00011667107700449856,
      "loss": 1.6739,
      "step": 3150
    },
    {
      "epoch": 0.836283039465414,
      "grad_norm": 0.9157118201255798,
      "learning_rate": 0.00011640645673458587,
      "loss": 1.7388,
      "step": 3160
    },
    {
      "epoch": 0.838929504780178,
      "grad_norm": 0.6909971833229065,
      "learning_rate": 0.0001161418364646732,
      "loss": 1.6442,
      "step": 3170
    },
    {
      "epoch": 0.8415759700949419,
      "grad_norm": 0.703937292098999,
      "learning_rate": 0.00011587721619476053,
      "loss": 1.7636,
      "step": 3180
    },
    {
      "epoch": 0.8442224354097059,
      "grad_norm": 0.7825332880020142,
      "learning_rate": 0.00011561259592484786,
      "loss": 1.8405,
      "step": 3190
    },
    {
      "epoch": 0.8468689007244699,
      "grad_norm": 0.9366546273231506,
      "learning_rate": 0.00011534797565493518,
      "loss": 1.8784,
      "step": 3200
    },
    {
      "epoch": 0.8495153660392338,
      "grad_norm": 0.666042149066925,
      "learning_rate": 0.00011508335538502251,
      "loss": 1.8026,
      "step": 3210
    },
    {
      "epoch": 0.8521618313539978,
      "grad_norm": 0.717581570148468,
      "learning_rate": 0.00011481873511510983,
      "loss": 1.7401,
      "step": 3220
    },
    {
      "epoch": 0.8548082966687618,
      "grad_norm": 0.6348410248756409,
      "learning_rate": 0.00011455411484519716,
      "loss": 1.7621,
      "step": 3230
    },
    {
      "epoch": 0.8574547619835258,
      "grad_norm": 0.8053573966026306,
      "learning_rate": 0.00011428949457528446,
      "loss": 1.7961,
      "step": 3240
    },
    {
      "epoch": 0.8601012272982897,
      "grad_norm": 0.7128551006317139,
      "learning_rate": 0.00011402487430537178,
      "loss": 1.8234,
      "step": 3250
    },
    {
      "epoch": 0.8627476926130537,
      "grad_norm": 0.7594432830810547,
      "learning_rate": 0.00011376025403545911,
      "loss": 1.776,
      "step": 3260
    },
    {
      "epoch": 0.8653941579278177,
      "grad_norm": 0.7694504857063293,
      "learning_rate": 0.00011349563376554644,
      "loss": 1.691,
      "step": 3270
    },
    {
      "epoch": 0.8680406232425816,
      "grad_norm": 0.6385884284973145,
      "learning_rate": 0.00011323101349563377,
      "loss": 1.7605,
      "step": 3280
    },
    {
      "epoch": 0.8706870885573456,
      "grad_norm": 0.7334691286087036,
      "learning_rate": 0.00011296639322572108,
      "loss": 1.7569,
      "step": 3290
    },
    {
      "epoch": 0.8733335538721095,
      "grad_norm": 0.6233516335487366,
      "learning_rate": 0.00011270177295580841,
      "loss": 1.8576,
      "step": 3300
    },
    {
      "epoch": 0.8759800191868735,
      "grad_norm": 0.6920576095581055,
      "learning_rate": 0.00011243715268589574,
      "loss": 1.7508,
      "step": 3310
    },
    {
      "epoch": 0.8786264845016375,
      "grad_norm": 0.618963360786438,
      "learning_rate": 0.00011217253241598307,
      "loss": 1.7444,
      "step": 3320
    },
    {
      "epoch": 0.8812729498164015,
      "grad_norm": 0.701813817024231,
      "learning_rate": 0.0001119079121460704,
      "loss": 1.702,
      "step": 3330
    },
    {
      "epoch": 0.8839194151311655,
      "grad_norm": 0.7535393238067627,
      "learning_rate": 0.00011164329187615771,
      "loss": 1.756,
      "step": 3340
    },
    {
      "epoch": 0.8865658804459294,
      "grad_norm": 0.7255748510360718,
      "learning_rate": 0.00011137867160624504,
      "loss": 1.7185,
      "step": 3350
    },
    {
      "epoch": 0.8892123457606934,
      "grad_norm": 0.7026023864746094,
      "learning_rate": 0.00011111405133633237,
      "loss": 1.7122,
      "step": 3360
    },
    {
      "epoch": 0.8918588110754574,
      "grad_norm": 0.8461330533027649,
      "learning_rate": 0.0001108494310664197,
      "loss": 1.8259,
      "step": 3370
    },
    {
      "epoch": 0.8945052763902213,
      "grad_norm": 0.7996046543121338,
      "learning_rate": 0.00011058481079650702,
      "loss": 1.8294,
      "step": 3380
    },
    {
      "epoch": 0.8971517417049852,
      "grad_norm": 0.7182318568229675,
      "learning_rate": 0.00011032019052659435,
      "loss": 1.7788,
      "step": 3390
    },
    {
      "epoch": 0.8997982070197492,
      "grad_norm": 0.7352997064590454,
      "learning_rate": 0.00011005557025668167,
      "loss": 1.8621,
      "step": 3400
    },
    {
      "epoch": 0.9024446723345132,
      "grad_norm": 0.6619837284088135,
      "learning_rate": 0.000109790949986769,
      "loss": 1.7848,
      "step": 3410
    },
    {
      "epoch": 0.9050911376492772,
      "grad_norm": 0.7699211239814758,
      "learning_rate": 0.00010952632971685632,
      "loss": 1.7561,
      "step": 3420
    },
    {
      "epoch": 0.9077376029640412,
      "grad_norm": 0.6913813352584839,
      "learning_rate": 0.00010926170944694365,
      "loss": 1.6991,
      "step": 3430
    },
    {
      "epoch": 0.9103840682788051,
      "grad_norm": 0.8168513178825378,
      "learning_rate": 0.00010899708917703098,
      "loss": 1.7375,
      "step": 3440
    },
    {
      "epoch": 0.9130305335935691,
      "grad_norm": 0.6226862668991089,
      "learning_rate": 0.0001087324689071183,
      "loss": 1.8077,
      "step": 3450
    },
    {
      "epoch": 0.9156769989083331,
      "grad_norm": 0.9964741468429565,
      "learning_rate": 0.00010846784863720561,
      "loss": 1.7276,
      "step": 3460
    },
    {
      "epoch": 0.9183234642230971,
      "grad_norm": 0.82342129945755,
      "learning_rate": 0.00010820322836729292,
      "loss": 1.8011,
      "step": 3470
    },
    {
      "epoch": 0.9209699295378609,
      "grad_norm": 0.6656497120857239,
      "learning_rate": 0.00010793860809738025,
      "loss": 1.7219,
      "step": 3480
    },
    {
      "epoch": 0.9236163948526249,
      "grad_norm": 0.6532673239707947,
      "learning_rate": 0.00010767398782746758,
      "loss": 1.7979,
      "step": 3490
    },
    {
      "epoch": 0.9262628601673889,
      "grad_norm": 0.697053074836731,
      "learning_rate": 0.00010740936755755491,
      "loss": 1.7465,
      "step": 3500
    },
    {
      "epoch": 0.9289093254821529,
      "grad_norm": 0.7495834827423096,
      "learning_rate": 0.00010714474728764224,
      "loss": 1.8122,
      "step": 3510
    },
    {
      "epoch": 0.9315557907969169,
      "grad_norm": 0.7638975381851196,
      "learning_rate": 0.00010688012701772955,
      "loss": 1.8383,
      "step": 3520
    },
    {
      "epoch": 0.9342022561116808,
      "grad_norm": 0.8237962126731873,
      "learning_rate": 0.00010661550674781688,
      "loss": 1.8251,
      "step": 3530
    },
    {
      "epoch": 0.9368487214264448,
      "grad_norm": 0.6363449692726135,
      "learning_rate": 0.00010635088647790421,
      "loss": 1.6727,
      "step": 3540
    },
    {
      "epoch": 0.9394951867412088,
      "grad_norm": 0.7227054238319397,
      "learning_rate": 0.00010608626620799154,
      "loss": 1.6838,
      "step": 3550
    },
    {
      "epoch": 0.9421416520559728,
      "grad_norm": 0.7596559524536133,
      "learning_rate": 0.00010582164593807887,
      "loss": 1.6765,
      "step": 3560
    },
    {
      "epoch": 0.9447881173707368,
      "grad_norm": 0.6852826476097107,
      "learning_rate": 0.0001055570256681662,
      "loss": 1.7709,
      "step": 3570
    },
    {
      "epoch": 0.9474345826855006,
      "grad_norm": 0.7200233936309814,
      "learning_rate": 0.00010529240539825351,
      "loss": 1.814,
      "step": 3580
    },
    {
      "epoch": 0.9500810480002646,
      "grad_norm": 0.7289181351661682,
      "learning_rate": 0.00010502778512834084,
      "loss": 1.7103,
      "step": 3590
    },
    {
      "epoch": 0.9527275133150286,
      "grad_norm": 0.8121880292892456,
      "learning_rate": 0.00010476316485842816,
      "loss": 1.8128,
      "step": 3600
    },
    {
      "epoch": 0.9553739786297926,
      "grad_norm": 0.7827505469322205,
      "learning_rate": 0.00010449854458851549,
      "loss": 1.8441,
      "step": 3610
    },
    {
      "epoch": 0.9580204439445565,
      "grad_norm": 0.6488521099090576,
      "learning_rate": 0.00010423392431860282,
      "loss": 1.7035,
      "step": 3620
    },
    {
      "epoch": 0.9606669092593205,
      "grad_norm": 0.903446614742279,
      "learning_rate": 0.00010396930404869014,
      "loss": 1.7406,
      "step": 3630
    },
    {
      "epoch": 0.9633133745740845,
      "grad_norm": 0.7474234700202942,
      "learning_rate": 0.00010370468377877746,
      "loss": 1.6815,
      "step": 3640
    },
    {
      "epoch": 0.9659598398888485,
      "grad_norm": 0.6042277216911316,
      "learning_rate": 0.00010344006350886479,
      "loss": 1.7529,
      "step": 3650
    },
    {
      "epoch": 0.9686063052036125,
      "grad_norm": 0.7853690385818481,
      "learning_rate": 0.00010317544323895212,
      "loss": 1.818,
      "step": 3660
    },
    {
      "epoch": 0.9712527705183764,
      "grad_norm": 0.818947970867157,
      "learning_rate": 0.00010291082296903945,
      "loss": 1.7649,
      "step": 3670
    },
    {
      "epoch": 0.9738992358331403,
      "grad_norm": 0.8832895159721375,
      "learning_rate": 0.00010264620269912675,
      "loss": 1.781,
      "step": 3680
    },
    {
      "epoch": 0.9765457011479043,
      "grad_norm": 0.6896909475326538,
      "learning_rate": 0.00010238158242921408,
      "loss": 1.7158,
      "step": 3690
    },
    {
      "epoch": 0.9791921664626683,
      "grad_norm": 0.7409948706626892,
      "learning_rate": 0.00010211696215930139,
      "loss": 1.7703,
      "step": 3700
    },
    {
      "epoch": 0.9818386317774322,
      "grad_norm": 0.8497593402862549,
      "learning_rate": 0.00010185234188938872,
      "loss": 1.7393,
      "step": 3710
    },
    {
      "epoch": 0.9844850970921962,
      "grad_norm": 0.9127022624015808,
      "learning_rate": 0.00010158772161947605,
      "loss": 1.8256,
      "step": 3720
    },
    {
      "epoch": 0.9871315624069602,
      "grad_norm": 0.6929238438606262,
      "learning_rate": 0.00010132310134956338,
      "loss": 1.7147,
      "step": 3730
    },
    {
      "epoch": 0.9897780277217242,
      "grad_norm": 0.7229201197624207,
      "learning_rate": 0.0001010584810796507,
      "loss": 1.8093,
      "step": 3740
    },
    {
      "epoch": 0.9924244930364882,
      "grad_norm": 0.6637963652610779,
      "learning_rate": 0.00010079386080973803,
      "loss": 1.7383,
      "step": 3750
    },
    {
      "epoch": 0.9950709583512521,
      "grad_norm": 0.8762044906616211,
      "learning_rate": 0.00010052924053982535,
      "loss": 1.7717,
      "step": 3760
    },
    {
      "epoch": 0.9977174236660161,
      "grad_norm": 0.760816216468811,
      "learning_rate": 0.00010026462026991268,
      "loss": 1.7307,
      "step": 3770
    },
    {
      "epoch": 1.0002646465314764,
      "grad_norm": 0.7012448906898499,
      "learning_rate": 0.0001,
      "loss": 1.631,
      "step": 3780
    },
    {
      "epoch": 1.0029111118462404,
      "grad_norm": 0.6220337152481079,
      "learning_rate": 9.973537973008733e-05,
      "loss": 1.6826,
      "step": 3790
    },
    {
      "epoch": 1.0055575771610044,
      "grad_norm": 0.6883073449134827,
      "learning_rate": 9.947075946017466e-05,
      "loss": 1.6921,
      "step": 3800
    },
    {
      "epoch": 1.0082040424757683,
      "grad_norm": 0.764370322227478,
      "learning_rate": 9.920613919026198e-05,
      "loss": 1.7558,
      "step": 3810
    },
    {
      "epoch": 1.0108505077905323,
      "grad_norm": 0.8988189101219177,
      "learning_rate": 9.89415189203493e-05,
      "loss": 1.6715,
      "step": 3820
    },
    {
      "epoch": 1.0134969731052963,
      "grad_norm": 0.8254611492156982,
      "learning_rate": 9.867689865043663e-05,
      "loss": 1.7591,
      "step": 3830
    },
    {
      "epoch": 1.0161434384200603,
      "grad_norm": 0.9052683711051941,
      "learning_rate": 9.841227838052395e-05,
      "loss": 1.7463,
      "step": 3840
    },
    {
      "epoch": 1.0187899037348243,
      "grad_norm": 0.6517639756202698,
      "learning_rate": 9.814765811061128e-05,
      "loss": 1.756,
      "step": 3850
    },
    {
      "epoch": 1.0214363690495882,
      "grad_norm": 0.7795677781105042,
      "learning_rate": 9.78830378406986e-05,
      "loss": 1.7079,
      "step": 3860
    },
    {
      "epoch": 1.0240828343643522,
      "grad_norm": 0.6203340888023376,
      "learning_rate": 9.761841757078593e-05,
      "loss": 1.6718,
      "step": 3870
    },
    {
      "epoch": 1.0267292996791162,
      "grad_norm": 0.635310173034668,
      "learning_rate": 9.735379730087325e-05,
      "loss": 1.755,
      "step": 3880
    },
    {
      "epoch": 1.02937576499388,
      "grad_norm": 0.7500247359275818,
      "learning_rate": 9.708917703096057e-05,
      "loss": 1.7404,
      "step": 3890
    },
    {
      "epoch": 1.032022230308644,
      "grad_norm": 0.7942167520523071,
      "learning_rate": 9.68245567610479e-05,
      "loss": 1.6903,
      "step": 3900
    },
    {
      "epoch": 1.034668695623408,
      "grad_norm": 0.6995306015014648,
      "learning_rate": 9.655993649113523e-05,
      "loss": 1.767,
      "step": 3910
    },
    {
      "epoch": 1.0373151609381719,
      "grad_norm": 0.6812857389450073,
      "learning_rate": 9.629531622122256e-05,
      "loss": 1.6816,
      "step": 3920
    },
    {
      "epoch": 1.0399616262529359,
      "grad_norm": 0.9518692493438721,
      "learning_rate": 9.603069595130987e-05,
      "loss": 1.7337,
      "step": 3930
    },
    {
      "epoch": 1.0426080915676998,
      "grad_norm": 0.8578660488128662,
      "learning_rate": 9.57660756813972e-05,
      "loss": 1.7807,
      "step": 3940
    },
    {
      "epoch": 1.0452545568824638,
      "grad_norm": 0.8116557598114014,
      "learning_rate": 9.550145541148452e-05,
      "loss": 1.7264,
      "step": 3950
    },
    {
      "epoch": 1.0479010221972278,
      "grad_norm": 0.7775804996490479,
      "learning_rate": 9.523683514157185e-05,
      "loss": 1.71,
      "step": 3960
    },
    {
      "epoch": 1.0505474875119918,
      "grad_norm": 0.6604698300361633,
      "learning_rate": 9.497221487165917e-05,
      "loss": 1.7467,
      "step": 3970
    },
    {
      "epoch": 1.0531939528267558,
      "grad_norm": 0.9100205898284912,
      "learning_rate": 9.47075946017465e-05,
      "loss": 1.7591,
      "step": 3980
    },
    {
      "epoch": 1.0558404181415197,
      "grad_norm": 0.7225393652915955,
      "learning_rate": 9.444297433183382e-05,
      "loss": 1.8482,
      "step": 3990
    },
    {
      "epoch": 1.0584868834562837,
      "grad_norm": 0.96748286485672,
      "learning_rate": 9.417835406192114e-05,
      "loss": 1.808,
      "step": 4000
    },
    {
      "epoch": 1.0611333487710477,
      "grad_norm": 0.7829973697662354,
      "learning_rate": 9.391373379200847e-05,
      "loss": 1.8562,
      "step": 4010
    },
    {
      "epoch": 1.0637798140858117,
      "grad_norm": 0.6517279744148254,
      "learning_rate": 9.36491135220958e-05,
      "loss": 1.7214,
      "step": 4020
    },
    {
      "epoch": 1.0664262794005757,
      "grad_norm": 0.8612064123153687,
      "learning_rate": 9.338449325218313e-05,
      "loss": 1.7792,
      "step": 4030
    },
    {
      "epoch": 1.0690727447153396,
      "grad_norm": 0.7395286560058594,
      "learning_rate": 9.311987298227046e-05,
      "loss": 1.7905,
      "step": 4040
    },
    {
      "epoch": 1.0717192100301036,
      "grad_norm": 0.64199298620224,
      "learning_rate": 9.285525271235777e-05,
      "loss": 1.6679,
      "step": 4050
    },
    {
      "epoch": 1.0743656753448676,
      "grad_norm": 0.6384854912757874,
      "learning_rate": 9.259063244244509e-05,
      "loss": 1.6974,
      "step": 4060
    },
    {
      "epoch": 1.0770121406596316,
      "grad_norm": 0.7285736203193665,
      "learning_rate": 9.232601217253241e-05,
      "loss": 1.653,
      "step": 4070
    },
    {
      "epoch": 1.0796586059743953,
      "grad_norm": 0.8191081285476685,
      "learning_rate": 9.206139190261974e-05,
      "loss": 1.6684,
      "step": 4080
    },
    {
      "epoch": 1.0823050712891593,
      "grad_norm": 0.735843300819397,
      "learning_rate": 9.179677163270707e-05,
      "loss": 1.6683,
      "step": 4090
    },
    {
      "epoch": 1.0849515366039233,
      "grad_norm": 0.7090817093849182,
      "learning_rate": 9.15321513627944e-05,
      "loss": 1.7579,
      "step": 4100
    },
    {
      "epoch": 1.0875980019186873,
      "grad_norm": 0.7408530116081238,
      "learning_rate": 9.126753109288171e-05,
      "loss": 1.8267,
      "step": 4110
    },
    {
      "epoch": 1.0902444672334513,
      "grad_norm": 0.7782154679298401,
      "learning_rate": 9.100291082296904e-05,
      "loss": 1.7241,
      "step": 4120
    },
    {
      "epoch": 1.0928909325482152,
      "grad_norm": 0.8131974935531616,
      "learning_rate": 9.073829055305637e-05,
      "loss": 1.8021,
      "step": 4130
    },
    {
      "epoch": 1.0955373978629792,
      "grad_norm": 0.7436083555221558,
      "learning_rate": 9.04736702831437e-05,
      "loss": 1.6819,
      "step": 4140
    },
    {
      "epoch": 1.0981838631777432,
      "grad_norm": 0.9348844289779663,
      "learning_rate": 9.020905001323103e-05,
      "loss": 1.773,
      "step": 4150
    },
    {
      "epoch": 1.1008303284925072,
      "grad_norm": 0.9156845211982727,
      "learning_rate": 8.994442974331834e-05,
      "loss": 1.6772,
      "step": 4160
    },
    {
      "epoch": 1.1034767938072712,
      "grad_norm": 0.8044249415397644,
      "learning_rate": 8.967980947340566e-05,
      "loss": 1.8092,
      "step": 4170
    },
    {
      "epoch": 1.1061232591220351,
      "grad_norm": 1.0560338497161865,
      "learning_rate": 8.941518920349298e-05,
      "loss": 1.6861,
      "step": 4180
    },
    {
      "epoch": 1.108769724436799,
      "grad_norm": 0.7435228228569031,
      "learning_rate": 8.915056893358031e-05,
      "loss": 1.722,
      "step": 4190
    },
    {
      "epoch": 1.111416189751563,
      "grad_norm": 0.8843109607696533,
      "learning_rate": 8.888594866366764e-05,
      "loss": 1.7338,
      "step": 4200
    },
    {
      "epoch": 1.114062655066327,
      "grad_norm": 0.7378778457641602,
      "learning_rate": 8.862132839375497e-05,
      "loss": 1.7133,
      "step": 4210
    },
    {
      "epoch": 1.116709120381091,
      "grad_norm": 0.776560366153717,
      "learning_rate": 8.83567081238423e-05,
      "loss": 1.7929,
      "step": 4220
    },
    {
      "epoch": 1.119355585695855,
      "grad_norm": 0.7891479730606079,
      "learning_rate": 8.809208785392961e-05,
      "loss": 1.7113,
      "step": 4230
    },
    {
      "epoch": 1.122002051010619,
      "grad_norm": 0.8282167315483093,
      "learning_rate": 8.782746758401694e-05,
      "loss": 1.7426,
      "step": 4240
    },
    {
      "epoch": 1.124648516325383,
      "grad_norm": 1.004866123199463,
      "learning_rate": 8.756284731410427e-05,
      "loss": 1.7415,
      "step": 4250
    },
    {
      "epoch": 1.127294981640147,
      "grad_norm": 0.7029340863227844,
      "learning_rate": 8.72982270441916e-05,
      "loss": 1.8272,
      "step": 4260
    },
    {
      "epoch": 1.129941446954911,
      "grad_norm": 0.6607045531272888,
      "learning_rate": 8.703360677427891e-05,
      "loss": 1.8262,
      "step": 4270
    },
    {
      "epoch": 1.132587912269675,
      "grad_norm": 0.6558355689048767,
      "learning_rate": 8.676898650436624e-05,
      "loss": 1.7336,
      "step": 4280
    },
    {
      "epoch": 1.135234377584439,
      "grad_norm": 0.9185397028923035,
      "learning_rate": 8.650436623445355e-05,
      "loss": 1.7538,
      "step": 4290
    },
    {
      "epoch": 1.1378808428992027,
      "grad_norm": 0.7011095881462097,
      "learning_rate": 8.623974596454088e-05,
      "loss": 1.7001,
      "step": 4300
    },
    {
      "epoch": 1.1405273082139666,
      "grad_norm": 0.7206640839576721,
      "learning_rate": 8.597512569462821e-05,
      "loss": 1.7259,
      "step": 4310
    },
    {
      "epoch": 1.1431737735287306,
      "grad_norm": 0.9881951808929443,
      "learning_rate": 8.571050542471554e-05,
      "loss": 1.8114,
      "step": 4320
    },
    {
      "epoch": 1.1458202388434946,
      "grad_norm": 0.6805316805839539,
      "learning_rate": 8.544588515480287e-05,
      "loss": 1.6728,
      "step": 4330
    },
    {
      "epoch": 1.1484667041582586,
      "grad_norm": 0.8163212537765503,
      "learning_rate": 8.518126488489018e-05,
      "loss": 1.6546,
      "step": 4340
    },
    {
      "epoch": 1.1511131694730226,
      "grad_norm": 0.7944371700286865,
      "learning_rate": 8.491664461497751e-05,
      "loss": 1.6694,
      "step": 4350
    },
    {
      "epoch": 1.1537596347877865,
      "grad_norm": 0.772851824760437,
      "learning_rate": 8.465202434506484e-05,
      "loss": 1.7902,
      "step": 4360
    },
    {
      "epoch": 1.1564061001025505,
      "grad_norm": 0.8008764386177063,
      "learning_rate": 8.438740407515217e-05,
      "loss": 1.7937,
      "step": 4370
    },
    {
      "epoch": 1.1590525654173145,
      "grad_norm": 0.752498984336853,
      "learning_rate": 8.412278380523948e-05,
      "loss": 1.7447,
      "step": 4380
    },
    {
      "epoch": 1.1616990307320785,
      "grad_norm": 0.778677761554718,
      "learning_rate": 8.385816353532681e-05,
      "loss": 1.7353,
      "step": 4390
    },
    {
      "epoch": 1.1643454960468425,
      "grad_norm": 0.701751708984375,
      "learning_rate": 8.359354326541414e-05,
      "loss": 1.6671,
      "step": 4400
    },
    {
      "epoch": 1.1669919613616064,
      "grad_norm": 0.7035423517227173,
      "learning_rate": 8.332892299550145e-05,
      "loss": 1.7513,
      "step": 4410
    },
    {
      "epoch": 1.1696384266763704,
      "grad_norm": 0.706741213798523,
      "learning_rate": 8.306430272558878e-05,
      "loss": 1.6895,
      "step": 4420
    },
    {
      "epoch": 1.1722848919911344,
      "grad_norm": 0.8078981637954712,
      "learning_rate": 8.279968245567611e-05,
      "loss": 1.7587,
      "step": 4430
    },
    {
      "epoch": 1.1749313573058984,
      "grad_norm": 0.9744340777397156,
      "learning_rate": 8.253506218576344e-05,
      "loss": 1.6583,
      "step": 4440
    },
    {
      "epoch": 1.1775778226206624,
      "grad_norm": 0.7264317274093628,
      "learning_rate": 8.227044191585077e-05,
      "loss": 1.7848,
      "step": 4450
    },
    {
      "epoch": 1.1802242879354263,
      "grad_norm": 0.9846979379653931,
      "learning_rate": 8.200582164593808e-05,
      "loss": 1.8035,
      "step": 4460
    },
    {
      "epoch": 1.1828707532501903,
      "grad_norm": 1.0908598899841309,
      "learning_rate": 8.174120137602541e-05,
      "loss": 1.8414,
      "step": 4470
    },
    {
      "epoch": 1.185517218564954,
      "grad_norm": 0.7660649418830872,
      "learning_rate": 8.147658110611274e-05,
      "loss": 1.7316,
      "step": 4480
    },
    {
      "epoch": 1.188163683879718,
      "grad_norm": 0.8359145522117615,
      "learning_rate": 8.121196083620005e-05,
      "loss": 1.6988,
      "step": 4490
    },
    {
      "epoch": 1.190810149194482,
      "grad_norm": 0.7861338257789612,
      "learning_rate": 8.094734056628738e-05,
      "loss": 1.7425,
      "step": 4500
    },
    {
      "epoch": 1.193456614509246,
      "grad_norm": 0.7710031867027283,
      "learning_rate": 8.068272029637471e-05,
      "loss": 1.7516,
      "step": 4510
    },
    {
      "epoch": 1.19610307982401,
      "grad_norm": 0.7391358613967896,
      "learning_rate": 8.041810002646202e-05,
      "loss": 1.8227,
      "step": 4520
    },
    {
      "epoch": 1.198749545138774,
      "grad_norm": 0.6401258111000061,
      "learning_rate": 8.015347975654935e-05,
      "loss": 1.792,
      "step": 4530
    },
    {
      "epoch": 1.201396010453538,
      "grad_norm": 0.6992084980010986,
      "learning_rate": 7.988885948663668e-05,
      "loss": 1.8388,
      "step": 4540
    },
    {
      "epoch": 1.204042475768302,
      "grad_norm": 0.7382891774177551,
      "learning_rate": 7.962423921672401e-05,
      "loss": 1.8614,
      "step": 4550
    },
    {
      "epoch": 1.206688941083066,
      "grad_norm": 0.8390880823135376,
      "learning_rate": 7.935961894681134e-05,
      "loss": 1.7531,
      "step": 4560
    },
    {
      "epoch": 1.2093354063978299,
      "grad_norm": 0.9125055074691772,
      "learning_rate": 7.909499867689866e-05,
      "loss": 1.8239,
      "step": 4570
    },
    {
      "epoch": 1.2119818717125939,
      "grad_norm": 0.6212892532348633,
      "learning_rate": 7.883037840698598e-05,
      "loss": 1.7772,
      "step": 4580
    },
    {
      "epoch": 1.2146283370273578,
      "grad_norm": 0.7423518896102905,
      "learning_rate": 7.85657581370733e-05,
      "loss": 1.7225,
      "step": 4590
    },
    {
      "epoch": 1.2172748023421218,
      "grad_norm": 0.7694671154022217,
      "learning_rate": 7.830113786716062e-05,
      "loss": 1.6787,
      "step": 4600
    },
    {
      "epoch": 1.2199212676568858,
      "grad_norm": 0.7721973657608032,
      "learning_rate": 7.803651759724795e-05,
      "loss": 1.7624,
      "step": 4610
    },
    {
      "epoch": 1.2225677329716498,
      "grad_norm": 1.0026763677597046,
      "learning_rate": 7.777189732733528e-05,
      "loss": 1.711,
      "step": 4620
    },
    {
      "epoch": 1.2252141982864138,
      "grad_norm": 0.8535763025283813,
      "learning_rate": 7.75072770574226e-05,
      "loss": 1.763,
      "step": 4630
    },
    {
      "epoch": 1.2278606636011777,
      "grad_norm": 0.7942925095558167,
      "learning_rate": 7.724265678750992e-05,
      "loss": 1.687,
      "step": 4640
    },
    {
      "epoch": 1.2305071289159417,
      "grad_norm": 0.8106153011322021,
      "learning_rate": 7.697803651759725e-05,
      "loss": 1.6904,
      "step": 4650
    },
    {
      "epoch": 1.2331535942307057,
      "grad_norm": 0.7503038644790649,
      "learning_rate": 7.671341624768458e-05,
      "loss": 1.6568,
      "step": 4660
    },
    {
      "epoch": 1.2358000595454697,
      "grad_norm": 0.6120603084564209,
      "learning_rate": 7.64487959777719e-05,
      "loss": 1.7255,
      "step": 4670
    },
    {
      "epoch": 1.2384465248602337,
      "grad_norm": 0.8665099143981934,
      "learning_rate": 7.618417570785923e-05,
      "loss": 1.6711,
      "step": 4680
    },
    {
      "epoch": 1.2410929901749976,
      "grad_norm": 0.7312929034233093,
      "learning_rate": 7.591955543794655e-05,
      "loss": 1.8454,
      "step": 4690
    },
    {
      "epoch": 1.2437394554897614,
      "grad_norm": 0.6432283520698547,
      "learning_rate": 7.565493516803388e-05,
      "loss": 1.6866,
      "step": 4700
    },
    {
      "epoch": 1.2463859208045254,
      "grad_norm": 0.7114768624305725,
      "learning_rate": 7.539031489812119e-05,
      "loss": 1.7022,
      "step": 4710
    },
    {
      "epoch": 1.2490323861192894,
      "grad_norm": 0.7492959499359131,
      "learning_rate": 7.512569462820852e-05,
      "loss": 1.6688,
      "step": 4720
    },
    {
      "epoch": 1.2516788514340533,
      "grad_norm": 0.6426481008529663,
      "learning_rate": 7.486107435829585e-05,
      "loss": 1.7167,
      "step": 4730
    },
    {
      "epoch": 1.2543253167488173,
      "grad_norm": 0.6562754511833191,
      "learning_rate": 7.459645408838318e-05,
      "loss": 1.7884,
      "step": 4740
    },
    {
      "epoch": 1.2569717820635813,
      "grad_norm": 0.6784733533859253,
      "learning_rate": 7.43318338184705e-05,
      "loss": 1.7736,
      "step": 4750
    },
    {
      "epoch": 1.2596182473783453,
      "grad_norm": 0.6762726306915283,
      "learning_rate": 7.406721354855782e-05,
      "loss": 1.6859,
      "step": 4760
    },
    {
      "epoch": 1.2622647126931092,
      "grad_norm": 0.7296660542488098,
      "learning_rate": 7.380259327864515e-05,
      "loss": 1.7149,
      "step": 4770
    },
    {
      "epoch": 1.2649111780078732,
      "grad_norm": 0.9474407434463501,
      "learning_rate": 7.353797300873248e-05,
      "loss": 1.7138,
      "step": 4780
    },
    {
      "epoch": 1.2675576433226372,
      "grad_norm": 0.7226319909095764,
      "learning_rate": 7.32733527388198e-05,
      "loss": 1.7232,
      "step": 4790
    },
    {
      "epoch": 1.2702041086374012,
      "grad_norm": 0.7266539335250854,
      "learning_rate": 7.300873246890713e-05,
      "loss": 1.7533,
      "step": 4800
    },
    {
      "epoch": 1.2728505739521652,
      "grad_norm": 0.88140869140625,
      "learning_rate": 7.274411219899445e-05,
      "loss": 1.706,
      "step": 4810
    },
    {
      "epoch": 1.2754970392669291,
      "grad_norm": 0.6442774534225464,
      "learning_rate": 7.247949192908176e-05,
      "loss": 1.7187,
      "step": 4820
    },
    {
      "epoch": 1.2781435045816931,
      "grad_norm": 0.6805208921432495,
      "learning_rate": 7.221487165916909e-05,
      "loss": 1.675,
      "step": 4830
    },
    {
      "epoch": 1.280789969896457,
      "grad_norm": 0.8185389041900635,
      "learning_rate": 7.195025138925642e-05,
      "loss": 1.7398,
      "step": 4840
    },
    {
      "epoch": 1.283436435211221,
      "grad_norm": 0.8905414938926697,
      "learning_rate": 7.168563111934375e-05,
      "loss": 1.731,
      "step": 4850
    },
    {
      "epoch": 1.286082900525985,
      "grad_norm": 0.7120075821876526,
      "learning_rate": 7.142101084943107e-05,
      "loss": 1.744,
      "step": 4860
    },
    {
      "epoch": 1.2887293658407488,
      "grad_norm": 0.8603763580322266,
      "learning_rate": 7.115639057951839e-05,
      "loss": 1.781,
      "step": 4870
    },
    {
      "epoch": 1.2913758311555128,
      "grad_norm": 0.7888190746307373,
      "learning_rate": 7.089177030960572e-05,
      "loss": 1.6899,
      "step": 4880
    },
    {
      "epoch": 1.2940222964702768,
      "grad_norm": 0.7589383721351624,
      "learning_rate": 7.062715003969304e-05,
      "loss": 1.7698,
      "step": 4890
    },
    {
      "epoch": 1.2966687617850408,
      "grad_norm": 0.8750256299972534,
      "learning_rate": 7.036252976978037e-05,
      "loss": 1.7438,
      "step": 4900
    },
    {
      "epoch": 1.2993152270998047,
      "grad_norm": 0.7723931670188904,
      "learning_rate": 7.00979094998677e-05,
      "loss": 1.6661,
      "step": 4910
    },
    {
      "epoch": 1.3019616924145687,
      "grad_norm": 0.5889249444007874,
      "learning_rate": 6.983328922995503e-05,
      "loss": 1.7226,
      "step": 4920
    },
    {
      "epoch": 1.3046081577293327,
      "grad_norm": 0.9455743432044983,
      "learning_rate": 6.956866896004234e-05,
      "loss": 1.7778,
      "step": 4930
    },
    {
      "epoch": 1.3072546230440967,
      "grad_norm": 0.992461085319519,
      "learning_rate": 6.930404869012966e-05,
      "loss": 1.7221,
      "step": 4940
    },
    {
      "epoch": 1.3099010883588607,
      "grad_norm": 0.9407698512077332,
      "learning_rate": 6.903942842021699e-05,
      "loss": 1.6733,
      "step": 4950
    },
    {
      "epoch": 1.3125475536736246,
      "grad_norm": 0.6452447772026062,
      "learning_rate": 6.877480815030432e-05,
      "loss": 1.8001,
      "step": 4960
    },
    {
      "epoch": 1.3151940189883886,
      "grad_norm": 0.6730455160140991,
      "learning_rate": 6.851018788039164e-05,
      "loss": 1.6786,
      "step": 4970
    },
    {
      "epoch": 1.3178404843031526,
      "grad_norm": 0.9973152279853821,
      "learning_rate": 6.824556761047897e-05,
      "loss": 1.7105,
      "step": 4980
    },
    {
      "epoch": 1.3204869496179166,
      "grad_norm": 0.736953616142273,
      "learning_rate": 6.798094734056629e-05,
      "loss": 1.7443,
      "step": 4990
    },
    {
      "epoch": 1.3231334149326806,
      "grad_norm": 0.7174205183982849,
      "learning_rate": 6.771632707065361e-05,
      "loss": 1.8225,
      "step": 5000
    },
    {
      "epoch": 1.3257798802474445,
      "grad_norm": 0.6974385976791382,
      "learning_rate": 6.745170680074094e-05,
      "loss": 1.6519,
      "step": 5010
    },
    {
      "epoch": 1.3284263455622085,
      "grad_norm": 0.8320436477661133,
      "learning_rate": 6.718708653082827e-05,
      "loss": 1.7657,
      "step": 5020
    },
    {
      "epoch": 1.3310728108769725,
      "grad_norm": 0.7184924483299255,
      "learning_rate": 6.692246626091559e-05,
      "loss": 1.6552,
      "step": 5030
    },
    {
      "epoch": 1.3337192761917365,
      "grad_norm": 0.7366752028465271,
      "learning_rate": 6.665784599100291e-05,
      "loss": 1.7189,
      "step": 5040
    },
    {
      "epoch": 1.3363657415065004,
      "grad_norm": 0.8214210867881775,
      "learning_rate": 6.639322572109023e-05,
      "loss": 1.7966,
      "step": 5050
    },
    {
      "epoch": 1.3390122068212644,
      "grad_norm": 0.6809362769126892,
      "learning_rate": 6.612860545117756e-05,
      "loss": 1.6205,
      "step": 5060
    },
    {
      "epoch": 1.3416586721360284,
      "grad_norm": 0.810236930847168,
      "learning_rate": 6.586398518126489e-05,
      "loss": 1.6599,
      "step": 5070
    },
    {
      "epoch": 1.3443051374507924,
      "grad_norm": 0.7658647298812866,
      "learning_rate": 6.559936491135221e-05,
      "loss": 1.856,
      "step": 5080
    },
    {
      "epoch": 1.3469516027655564,
      "grad_norm": 0.8947018980979919,
      "learning_rate": 6.533474464143954e-05,
      "loss": 1.8221,
      "step": 5090
    },
    {
      "epoch": 1.3495980680803203,
      "grad_norm": 0.7542850375175476,
      "learning_rate": 6.507012437152687e-05,
      "loss": 1.6857,
      "step": 5100
    },
    {
      "epoch": 1.3522445333950843,
      "grad_norm": 0.7480062246322632,
      "learning_rate": 6.480550410161418e-05,
      "loss": 1.6463,
      "step": 5110
    },
    {
      "epoch": 1.354890998709848,
      "grad_norm": 0.8806074857711792,
      "learning_rate": 6.454088383170151e-05,
      "loss": 1.727,
      "step": 5120
    },
    {
      "epoch": 1.357537464024612,
      "grad_norm": 0.7506317496299744,
      "learning_rate": 6.427626356178884e-05,
      "loss": 1.7132,
      "step": 5130
    },
    {
      "epoch": 1.360183929339376,
      "grad_norm": 0.753696858882904,
      "learning_rate": 6.401164329187616e-05,
      "loss": 1.6406,
      "step": 5140
    },
    {
      "epoch": 1.36283039465414,
      "grad_norm": 0.7899975776672363,
      "learning_rate": 6.374702302196348e-05,
      "loss": 1.6884,
      "step": 5150
    },
    {
      "epoch": 1.365476859968904,
      "grad_norm": 0.8621976971626282,
      "learning_rate": 6.348240275205081e-05,
      "loss": 1.7368,
      "step": 5160
    },
    {
      "epoch": 1.368123325283668,
      "grad_norm": 1.1163153648376465,
      "learning_rate": 6.321778248213813e-05,
      "loss": 1.7687,
      "step": 5170
    },
    {
      "epoch": 1.370769790598432,
      "grad_norm": 0.6903082132339478,
      "learning_rate": 6.295316221222546e-05,
      "loss": 1.8389,
      "step": 5180
    },
    {
      "epoch": 1.373416255913196,
      "grad_norm": 0.8555641770362854,
      "learning_rate": 6.268854194231278e-05,
      "loss": 1.6523,
      "step": 5190
    },
    {
      "epoch": 1.37606272122796,
      "grad_norm": 0.9326136112213135,
      "learning_rate": 6.242392167240011e-05,
      "loss": 1.7562,
      "step": 5200
    },
    {
      "epoch": 1.378709186542724,
      "grad_norm": 0.5858316421508789,
      "learning_rate": 6.215930140248744e-05,
      "loss": 1.7556,
      "step": 5210
    },
    {
      "epoch": 1.3813556518574879,
      "grad_norm": 0.9187765121459961,
      "learning_rate": 6.189468113257475e-05,
      "loss": 1.6369,
      "step": 5220
    },
    {
      "epoch": 1.3840021171722519,
      "grad_norm": 0.7180696129798889,
      "learning_rate": 6.163006086266208e-05,
      "loss": 1.7505,
      "step": 5230
    },
    {
      "epoch": 1.3866485824870158,
      "grad_norm": 0.8747619390487671,
      "learning_rate": 6.136544059274941e-05,
      "loss": 1.8481,
      "step": 5240
    },
    {
      "epoch": 1.3892950478017798,
      "grad_norm": 0.8447882533073425,
      "learning_rate": 6.110082032283673e-05,
      "loss": 1.7152,
      "step": 5250
    },
    {
      "epoch": 1.3919415131165438,
      "grad_norm": 0.7747119069099426,
      "learning_rate": 6.0836200052924054e-05,
      "loss": 1.7364,
      "step": 5260
    },
    {
      "epoch": 1.3945879784313076,
      "grad_norm": 0.8088110089302063,
      "learning_rate": 6.0571579783011375e-05,
      "loss": 1.7332,
      "step": 5270
    },
    {
      "epoch": 1.3972344437460715,
      "grad_norm": 0.9785022735595703,
      "learning_rate": 6.0306959513098703e-05,
      "loss": 1.5996,
      "step": 5280
    },
    {
      "epoch": 1.3998809090608355,
      "grad_norm": 0.636475682258606,
      "learning_rate": 6.004233924318603e-05,
      "loss": 1.7136,
      "step": 5290
    },
    {
      "epoch": 1.4025273743755995,
      "grad_norm": 0.8489970564842224,
      "learning_rate": 5.977771897327335e-05,
      "loss": 1.7548,
      "step": 5300
    },
    {
      "epoch": 1.4051738396903635,
      "grad_norm": 0.9800618290901184,
      "learning_rate": 5.951309870336068e-05,
      "loss": 1.7079,
      "step": 5310
    },
    {
      "epoch": 1.4078203050051274,
      "grad_norm": 0.7655712366104126,
      "learning_rate": 5.924847843344801e-05,
      "loss": 1.7753,
      "step": 5320
    },
    {
      "epoch": 1.4104667703198914,
      "grad_norm": 0.738731861114502,
      "learning_rate": 5.898385816353533e-05,
      "loss": 1.7808,
      "step": 5330
    },
    {
      "epoch": 1.4131132356346554,
      "grad_norm": 0.8396647572517395,
      "learning_rate": 5.871923789362266e-05,
      "loss": 1.7581,
      "step": 5340
    },
    {
      "epoch": 1.4157597009494194,
      "grad_norm": 0.8632907867431641,
      "learning_rate": 5.845461762370998e-05,
      "loss": 1.7256,
      "step": 5350
    },
    {
      "epoch": 1.4184061662641834,
      "grad_norm": 0.9180960655212402,
      "learning_rate": 5.8189997353797295e-05,
      "loss": 1.7775,
      "step": 5360
    },
    {
      "epoch": 1.4210526315789473,
      "grad_norm": 0.7641739845275879,
      "learning_rate": 5.7925377083884624e-05,
      "loss": 1.7234,
      "step": 5370
    },
    {
      "epoch": 1.4236990968937113,
      "grad_norm": 0.836284339427948,
      "learning_rate": 5.766075681397195e-05,
      "loss": 1.6562,
      "step": 5380
    },
    {
      "epoch": 1.4263455622084753,
      "grad_norm": 0.7387949228286743,
      "learning_rate": 5.739613654405927e-05,
      "loss": 1.7151,
      "step": 5390
    },
    {
      "epoch": 1.4289920275232393,
      "grad_norm": 0.7201151251792908,
      "learning_rate": 5.71315162741466e-05,
      "loss": 1.6803,
      "step": 5400
    },
    {
      "epoch": 1.4316384928380033,
      "grad_norm": 0.7073637247085571,
      "learning_rate": 5.686689600423393e-05,
      "loss": 1.7862,
      "step": 5410
    },
    {
      "epoch": 1.4342849581527672,
      "grad_norm": 0.720869243144989,
      "learning_rate": 5.660227573432125e-05,
      "loss": 1.693,
      "step": 5420
    },
    {
      "epoch": 1.4369314234675312,
      "grad_norm": 0.9100102186203003,
      "learning_rate": 5.633765546440858e-05,
      "loss": 1.7993,
      "step": 5430
    },
    {
      "epoch": 1.4395778887822952,
      "grad_norm": 0.6468073725700378,
      "learning_rate": 5.60730351944959e-05,
      "loss": 1.7403,
      "step": 5440
    },
    {
      "epoch": 1.4422243540970592,
      "grad_norm": 0.7969008684158325,
      "learning_rate": 5.580841492458323e-05,
      "loss": 1.723,
      "step": 5450
    },
    {
      "epoch": 1.4448708194118232,
      "grad_norm": 0.8400229811668396,
      "learning_rate": 5.554379465467056e-05,
      "loss": 1.7329,
      "step": 5460
    },
    {
      "epoch": 1.4475172847265871,
      "grad_norm": 0.8327164649963379,
      "learning_rate": 5.527917438475787e-05,
      "loss": 1.6966,
      "step": 5470
    },
    {
      "epoch": 1.4501637500413511,
      "grad_norm": 0.7029672265052795,
      "learning_rate": 5.5014554114845193e-05,
      "loss": 1.7615,
      "step": 5480
    },
    {
      "epoch": 1.452810215356115,
      "grad_norm": 0.8531699180603027,
      "learning_rate": 5.474993384493252e-05,
      "loss": 1.7077,
      "step": 5490
    },
    {
      "epoch": 1.455456680670879,
      "grad_norm": 0.795355498790741,
      "learning_rate": 5.448531357501985e-05,
      "loss": 1.6614,
      "step": 5500
    },
    {
      "epoch": 1.458103145985643,
      "grad_norm": 0.6344528794288635,
      "learning_rate": 5.422069330510717e-05,
      "loss": 1.7621,
      "step": 5510
    },
    {
      "epoch": 1.4607496113004068,
      "grad_norm": 0.8317959308624268,
      "learning_rate": 5.39560730351945e-05,
      "loss": 1.8445,
      "step": 5520
    },
    {
      "epoch": 1.4633960766151708,
      "grad_norm": 0.6956526041030884,
      "learning_rate": 5.369145276528182e-05,
      "loss": 1.7506,
      "step": 5530
    },
    {
      "epoch": 1.4660425419299348,
      "grad_norm": 0.9443928599357605,
      "learning_rate": 5.342683249536915e-05,
      "loss": 1.7105,
      "step": 5540
    },
    {
      "epoch": 1.4686890072446988,
      "grad_norm": 0.6906945109367371,
      "learning_rate": 5.316221222545648e-05,
      "loss": 1.7958,
      "step": 5550
    },
    {
      "epoch": 1.4713354725594627,
      "grad_norm": 0.7352386713027954,
      "learning_rate": 5.28975919555438e-05,
      "loss": 1.711,
      "step": 5560
    },
    {
      "epoch": 1.4739819378742267,
      "grad_norm": 0.7155373692512512,
      "learning_rate": 5.263297168563113e-05,
      "loss": 1.6984,
      "step": 5570
    },
    {
      "epoch": 1.4766284031889907,
      "grad_norm": 0.8384057283401489,
      "learning_rate": 5.236835141571844e-05,
      "loss": 1.747,
      "step": 5580
    },
    {
      "epoch": 1.4792748685037547,
      "grad_norm": 0.5994724035263062,
      "learning_rate": 5.210373114580577e-05,
      "loss": 1.7058,
      "step": 5590
    },
    {
      "epoch": 1.4819213338185186,
      "grad_norm": 0.876326858997345,
      "learning_rate": 5.183911087589309e-05,
      "loss": 1.7218,
      "step": 5600
    },
    {
      "epoch": 1.4845677991332826,
      "grad_norm": 0.6802533268928528,
      "learning_rate": 5.157449060598042e-05,
      "loss": 1.7231,
      "step": 5610
    },
    {
      "epoch": 1.4872142644480466,
      "grad_norm": 0.5924605131149292,
      "learning_rate": 5.130987033606775e-05,
      "loss": 1.7196,
      "step": 5620
    },
    {
      "epoch": 1.4898607297628106,
      "grad_norm": 0.6452488899230957,
      "learning_rate": 5.104525006615507e-05,
      "loss": 1.7698,
      "step": 5630
    },
    {
      "epoch": 1.4925071950775746,
      "grad_norm": 0.8482314944267273,
      "learning_rate": 5.07806297962424e-05,
      "loss": 1.7063,
      "step": 5640
    },
    {
      "epoch": 1.4951536603923385,
      "grad_norm": 0.6793737411499023,
      "learning_rate": 5.051600952632972e-05,
      "loss": 1.7029,
      "step": 5650
    },
    {
      "epoch": 1.4978001257071025,
      "grad_norm": 0.7626010775566101,
      "learning_rate": 5.025138925641705e-05,
      "loss": 1.7926,
      "step": 5660
    },
    {
      "epoch": 1.5004465910218663,
      "grad_norm": 1.1233271360397339,
      "learning_rate": 4.998676898650437e-05,
      "loss": 1.6654,
      "step": 5670
    },
    {
      "epoch": 1.5030930563366303,
      "grad_norm": 0.7193592190742493,
      "learning_rate": 4.972214871659169e-05,
      "loss": 1.7187,
      "step": 5680
    },
    {
      "epoch": 1.5057395216513942,
      "grad_norm": 0.6453953385353088,
      "learning_rate": 4.945752844667902e-05,
      "loss": 1.6498,
      "step": 5690
    },
    {
      "epoch": 1.5083859869661582,
      "grad_norm": 1.0409715175628662,
      "learning_rate": 4.919290817676635e-05,
      "loss": 1.711,
      "step": 5700
    },
    {
      "epoch": 1.5110324522809222,
      "grad_norm": 0.835018515586853,
      "learning_rate": 4.892828790685367e-05,
      "loss": 1.772,
      "step": 5710
    },
    {
      "epoch": 1.5136789175956862,
      "grad_norm": 0.8204277157783508,
      "learning_rate": 4.866366763694099e-05,
      "loss": 1.7211,
      "step": 5720
    },
    {
      "epoch": 1.5163253829104502,
      "grad_norm": 0.8613850474357605,
      "learning_rate": 4.839904736702832e-05,
      "loss": 1.7611,
      "step": 5730
    },
    {
      "epoch": 1.5189718482252141,
      "grad_norm": 0.8283793330192566,
      "learning_rate": 4.813442709711564e-05,
      "loss": 1.7603,
      "step": 5740
    },
    {
      "epoch": 1.5216183135399781,
      "grad_norm": 0.7930801510810852,
      "learning_rate": 4.786980682720297e-05,
      "loss": 1.7071,
      "step": 5750
    },
    {
      "epoch": 1.524264778854742,
      "grad_norm": 0.6849656701087952,
      "learning_rate": 4.7605186557290296e-05,
      "loss": 1.7536,
      "step": 5760
    },
    {
      "epoch": 1.526911244169506,
      "grad_norm": 0.8159026503562927,
      "learning_rate": 4.734056628737761e-05,
      "loss": 1.8259,
      "step": 5770
    },
    {
      "epoch": 1.52955770948427,
      "grad_norm": 0.7942892909049988,
      "learning_rate": 4.707594601746494e-05,
      "loss": 1.7424,
      "step": 5780
    },
    {
      "epoch": 1.532204174799034,
      "grad_norm": 0.7684528231620789,
      "learning_rate": 4.681132574755227e-05,
      "loss": 1.7028,
      "step": 5790
    },
    {
      "epoch": 1.534850640113798,
      "grad_norm": 0.8054279685020447,
      "learning_rate": 4.654670547763959e-05,
      "loss": 1.7602,
      "step": 5800
    },
    {
      "epoch": 1.537497105428562,
      "grad_norm": 0.8489859104156494,
      "learning_rate": 4.6282085207726917e-05,
      "loss": 1.7414,
      "step": 5810
    },
    {
      "epoch": 1.540143570743326,
      "grad_norm": 0.7886384129524231,
      "learning_rate": 4.601746493781424e-05,
      "loss": 1.8085,
      "step": 5820
    },
    {
      "epoch": 1.54279003605809,
      "grad_norm": 0.7971821427345276,
      "learning_rate": 4.575284466790156e-05,
      "loss": 1.7105,
      "step": 5830
    },
    {
      "epoch": 1.545436501372854,
      "grad_norm": 0.6302488446235657,
      "learning_rate": 4.548822439798889e-05,
      "loss": 1.7199,
      "step": 5840
    },
    {
      "epoch": 1.548082966687618,
      "grad_norm": 0.7585299611091614,
      "learning_rate": 4.5223604128076216e-05,
      "loss": 1.769,
      "step": 5850
    },
    {
      "epoch": 1.550729432002382,
      "grad_norm": 0.7381268739700317,
      "learning_rate": 4.495898385816354e-05,
      "loss": 1.6942,
      "step": 5860
    },
    {
      "epoch": 1.5533758973171459,
      "grad_norm": 0.7251253724098206,
      "learning_rate": 4.4694363588250866e-05,
      "loss": 1.6655,
      "step": 5870
    },
    {
      "epoch": 1.5560223626319098,
      "grad_norm": 0.8881078362464905,
      "learning_rate": 4.442974331833819e-05,
      "loss": 1.7353,
      "step": 5880
    },
    {
      "epoch": 1.5586688279466738,
      "grad_norm": 0.759876549243927,
      "learning_rate": 4.416512304842551e-05,
      "loss": 1.7107,
      "step": 5890
    },
    {
      "epoch": 1.5613152932614378,
      "grad_norm": 0.8462802767753601,
      "learning_rate": 4.390050277851284e-05,
      "loss": 1.801,
      "step": 5900
    },
    {
      "epoch": 1.5639617585762018,
      "grad_norm": 0.8273951411247253,
      "learning_rate": 4.3635882508600165e-05,
      "loss": 1.6097,
      "step": 5910
    },
    {
      "epoch": 1.5666082238909658,
      "grad_norm": 0.8176769018173218,
      "learning_rate": 4.3371262238687486e-05,
      "loss": 1.6706,
      "step": 5920
    },
    {
      "epoch": 1.5692546892057297,
      "grad_norm": 0.8096449971199036,
      "learning_rate": 4.310664196877481e-05,
      "loss": 1.7852,
      "step": 5930
    },
    {
      "epoch": 1.5719011545204937,
      "grad_norm": 0.805481493473053,
      "learning_rate": 4.2842021698862136e-05,
      "loss": 1.7982,
      "step": 5940
    },
    {
      "epoch": 1.5745476198352575,
      "grad_norm": 0.8805009126663208,
      "learning_rate": 4.257740142894946e-05,
      "loss": 1.6877,
      "step": 5950
    },
    {
      "epoch": 1.5771940851500215,
      "grad_norm": 0.9031485915184021,
      "learning_rate": 4.2312781159036786e-05,
      "loss": 1.6148,
      "step": 5960
    },
    {
      "epoch": 1.5798405504647854,
      "grad_norm": 0.8634531497955322,
      "learning_rate": 4.2048160889124114e-05,
      "loss": 1.6749,
      "step": 5970
    },
    {
      "epoch": 1.5824870157795494,
      "grad_norm": 1.1417571306228638,
      "learning_rate": 4.178354061921143e-05,
      "loss": 1.6565,
      "step": 5980
    },
    {
      "epoch": 1.5851334810943134,
      "grad_norm": 0.7740825414657593,
      "learning_rate": 4.151892034929876e-05,
      "loss": 1.7652,
      "step": 5990
    },
    {
      "epoch": 1.5877799464090774,
      "grad_norm": 0.6063699722290039,
      "learning_rate": 4.1254300079386085e-05,
      "loss": 1.8077,
      "step": 6000
    },
    {
      "epoch": 1.5904264117238414,
      "grad_norm": 1.0011337995529175,
      "learning_rate": 4.0989679809473407e-05,
      "loss": 1.7659,
      "step": 6010
    },
    {
      "epoch": 1.5930728770386053,
      "grad_norm": 0.6957335472106934,
      "learning_rate": 4.0725059539560735e-05,
      "loss": 1.6745,
      "step": 6020
    },
    {
      "epoch": 1.5957193423533693,
      "grad_norm": 0.9611213803291321,
      "learning_rate": 4.0460439269648056e-05,
      "loss": 1.6569,
      "step": 6030
    },
    {
      "epoch": 1.5983658076681333,
      "grad_norm": 0.7006465196609497,
      "learning_rate": 4.019581899973538e-05,
      "loss": 1.7654,
      "step": 6040
    },
    {
      "epoch": 1.601012272982897,
      "grad_norm": 1.0154179334640503,
      "learning_rate": 3.9931198729822706e-05,
      "loss": 1.7094,
      "step": 6050
    },
    {
      "epoch": 1.603658738297661,
      "grad_norm": 0.7100669741630554,
      "learning_rate": 3.9666578459910034e-05,
      "loss": 1.6919,
      "step": 6060
    },
    {
      "epoch": 1.606305203612425,
      "grad_norm": 0.8339712023735046,
      "learning_rate": 3.9401958189997356e-05,
      "loss": 1.7566,
      "step": 6070
    },
    {
      "epoch": 1.608951668927189,
      "grad_norm": 0.9026548862457275,
      "learning_rate": 3.9137337920084684e-05,
      "loss": 1.6683,
      "step": 6080
    },
    {
      "epoch": 1.611598134241953,
      "grad_norm": 0.7517582774162292,
      "learning_rate": 3.8872717650172005e-05,
      "loss": 1.7667,
      "step": 6090
    },
    {
      "epoch": 1.614244599556717,
      "grad_norm": 0.9131317734718323,
      "learning_rate": 3.860809738025933e-05,
      "loss": 1.7758,
      "step": 6100
    },
    {
      "epoch": 1.616891064871481,
      "grad_norm": 0.9913753271102905,
      "learning_rate": 3.8343477110346655e-05,
      "loss": 1.6656,
      "step": 6110
    },
    {
      "epoch": 1.619537530186245,
      "grad_norm": 0.7545080780982971,
      "learning_rate": 3.8078856840433976e-05,
      "loss": 1.7621,
      "step": 6120
    },
    {
      "epoch": 1.622183995501009,
      "grad_norm": 0.6922537088394165,
      "learning_rate": 3.7814236570521305e-05,
      "loss": 1.7967,
      "step": 6130
    },
    {
      "epoch": 1.6248304608157729,
      "grad_norm": 0.7564741969108582,
      "learning_rate": 3.754961630060863e-05,
      "loss": 1.7303,
      "step": 6140
    },
    {
      "epoch": 1.6274769261305368,
      "grad_norm": 0.8700482249259949,
      "learning_rate": 3.7284996030695954e-05,
      "loss": 1.7714,
      "step": 6150
    },
    {
      "epoch": 1.6301233914453008,
      "grad_norm": 0.9469581246376038,
      "learning_rate": 3.7020375760783276e-05,
      "loss": 1.7206,
      "step": 6160
    },
    {
      "epoch": 1.6327698567600648,
      "grad_norm": 0.7866131663322449,
      "learning_rate": 3.6755755490870604e-05,
      "loss": 1.8107,
      "step": 6170
    },
    {
      "epoch": 1.6354163220748288,
      "grad_norm": 0.5904785990715027,
      "learning_rate": 3.6491135220957925e-05,
      "loss": 1.6604,
      "step": 6180
    },
    {
      "epoch": 1.6380627873895928,
      "grad_norm": 0.727042555809021,
      "learning_rate": 3.6226514951045254e-05,
      "loss": 1.6186,
      "step": 6190
    },
    {
      "epoch": 1.6407092527043567,
      "grad_norm": 0.8408060073852539,
      "learning_rate": 3.5961894681132575e-05,
      "loss": 1.688,
      "step": 6200
    },
    {
      "epoch": 1.6433557180191207,
      "grad_norm": 0.8613675832748413,
      "learning_rate": 3.5697274411219897e-05,
      "loss": 1.7377,
      "step": 6210
    },
    {
      "epoch": 1.6460021833338847,
      "grad_norm": 0.8290298581123352,
      "learning_rate": 3.5432654141307225e-05,
      "loss": 1.6204,
      "step": 6220
    },
    {
      "epoch": 1.6486486486486487,
      "grad_norm": 0.7903677821159363,
      "learning_rate": 3.516803387139455e-05,
      "loss": 1.7471,
      "step": 6230
    },
    {
      "epoch": 1.6512951139634127,
      "grad_norm": 0.71366286277771,
      "learning_rate": 3.4903413601481874e-05,
      "loss": 1.7146,
      "step": 6240
    },
    {
      "epoch": 1.6539415792781766,
      "grad_norm": 0.6972216367721558,
      "learning_rate": 3.46387933315692e-05,
      "loss": 1.7309,
      "step": 6250
    },
    {
      "epoch": 1.6565880445929406,
      "grad_norm": 0.7236323356628418,
      "learning_rate": 3.4374173061656524e-05,
      "loss": 1.7852,
      "step": 6260
    },
    {
      "epoch": 1.6592345099077046,
      "grad_norm": 0.7920033931732178,
      "learning_rate": 3.4109552791743846e-05,
      "loss": 1.7157,
      "step": 6270
    },
    {
      "epoch": 1.6618809752224686,
      "grad_norm": 0.7631082534790039,
      "learning_rate": 3.3844932521831174e-05,
      "loss": 1.6875,
      "step": 6280
    },
    {
      "epoch": 1.6645274405372326,
      "grad_norm": 0.7632799744606018,
      "learning_rate": 3.35803122519185e-05,
      "loss": 1.8022,
      "step": 6290
    },
    {
      "epoch": 1.6671739058519965,
      "grad_norm": 0.8152360916137695,
      "learning_rate": 3.3315691982005823e-05,
      "loss": 1.722,
      "step": 6300
    },
    {
      "epoch": 1.6698203711667605,
      "grad_norm": 1.107284426689148,
      "learning_rate": 3.3051071712093145e-05,
      "loss": 1.7075,
      "step": 6310
    },
    {
      "epoch": 1.6724668364815245,
      "grad_norm": 0.7551271915435791,
      "learning_rate": 3.278645144218047e-05,
      "loss": 1.799,
      "step": 6320
    },
    {
      "epoch": 1.6751133017962885,
      "grad_norm": 0.8993650078773499,
      "learning_rate": 3.2521831172267795e-05,
      "loss": 1.7454,
      "step": 6330
    },
    {
      "epoch": 1.6777597671110525,
      "grad_norm": 0.7418166399002075,
      "learning_rate": 3.225721090235512e-05,
      "loss": 1.6915,
      "step": 6340
    },
    {
      "epoch": 1.6804062324258162,
      "grad_norm": 0.901351809501648,
      "learning_rate": 3.199259063244245e-05,
      "loss": 1.6546,
      "step": 6350
    },
    {
      "epoch": 1.6830526977405802,
      "grad_norm": 0.6970906853675842,
      "learning_rate": 3.1727970362529766e-05,
      "loss": 1.761,
      "step": 6360
    },
    {
      "epoch": 1.6856991630553442,
      "grad_norm": 0.9675236344337463,
      "learning_rate": 3.1463350092617094e-05,
      "loss": 1.7663,
      "step": 6370
    },
    {
      "epoch": 1.6883456283701082,
      "grad_norm": 0.8301258087158203,
      "learning_rate": 3.119872982270442e-05,
      "loss": 1.6762,
      "step": 6380
    },
    {
      "epoch": 1.6909920936848721,
      "grad_norm": 0.7713663578033447,
      "learning_rate": 3.0934109552791744e-05,
      "loss": 1.784,
      "step": 6390
    },
    {
      "epoch": 1.6936385589996361,
      "grad_norm": 0.8123850226402283,
      "learning_rate": 3.066948928287907e-05,
      "loss": 1.715,
      "step": 6400
    },
    {
      "epoch": 1.6962850243144,
      "grad_norm": 0.9123881459236145,
      "learning_rate": 3.0404869012966397e-05,
      "loss": 1.7983,
      "step": 6410
    },
    {
      "epoch": 1.698931489629164,
      "grad_norm": 0.7270055413246155,
      "learning_rate": 3.0140248743053718e-05,
      "loss": 1.7639,
      "step": 6420
    },
    {
      "epoch": 1.701577954943928,
      "grad_norm": 0.7861279249191284,
      "learning_rate": 2.9875628473141043e-05,
      "loss": 1.5835,
      "step": 6430
    },
    {
      "epoch": 1.704224420258692,
      "grad_norm": 0.7775775790214539,
      "learning_rate": 2.9611008203228368e-05,
      "loss": 1.7823,
      "step": 6440
    },
    {
      "epoch": 1.7068708855734558,
      "grad_norm": 0.8044657707214355,
      "learning_rate": 2.9346387933315693e-05,
      "loss": 1.6522,
      "step": 6450
    },
    {
      "epoch": 1.7095173508882198,
      "grad_norm": 0.8185216188430786,
      "learning_rate": 2.908176766340302e-05,
      "loss": 1.7729,
      "step": 6460
    },
    {
      "epoch": 1.7121638162029837,
      "grad_norm": 0.7134133577346802,
      "learning_rate": 2.881714739349034e-05,
      "loss": 1.6905,
      "step": 6470
    },
    {
      "epoch": 1.7148102815177477,
      "grad_norm": 0.7260785698890686,
      "learning_rate": 2.8552527123577667e-05,
      "loss": 1.7036,
      "step": 6480
    },
    {
      "epoch": 1.7174567468325117,
      "grad_norm": 0.8630152344703674,
      "learning_rate": 2.8287906853664992e-05,
      "loss": 1.7891,
      "step": 6490
    },
    {
      "epoch": 1.7201032121472757,
      "grad_norm": 0.746364951133728,
      "learning_rate": 2.8023286583752317e-05,
      "loss": 1.6876,
      "step": 6500
    },
    {
      "epoch": 1.7227496774620397,
      "grad_norm": 0.8816819787025452,
      "learning_rate": 2.7758666313839642e-05,
      "loss": 1.7117,
      "step": 6510
    },
    {
      "epoch": 1.7253961427768036,
      "grad_norm": 0.688913106918335,
      "learning_rate": 2.7520508070918234e-05,
      "loss": 1.7348,
      "step": 6520
    },
    {
      "epoch": 1.7280426080915676,
      "grad_norm": 0.6549805402755737,
      "learning_rate": 2.725588780100556e-05,
      "loss": 1.6606,
      "step": 6530
    },
    {
      "epoch": 1.7306890734063316,
      "grad_norm": 0.7797085642814636,
      "learning_rate": 2.6991267531092883e-05,
      "loss": 1.7305,
      "step": 6540
    },
    {
      "epoch": 1.7333355387210956,
      "grad_norm": 1.170574426651001,
      "learning_rate": 2.6726647261180208e-05,
      "loss": 1.6281,
      "step": 6550
    },
    {
      "epoch": 1.7359820040358596,
      "grad_norm": 0.9584737420082092,
      "learning_rate": 2.646202699126753e-05,
      "loss": 1.7381,
      "step": 6560
    },
    {
      "epoch": 1.7386284693506235,
      "grad_norm": 0.716942310333252,
      "learning_rate": 2.6197406721354855e-05,
      "loss": 1.8082,
      "step": 6570
    },
    {
      "epoch": 1.7412749346653875,
      "grad_norm": 0.7698225975036621,
      "learning_rate": 2.5932786451442183e-05,
      "loss": 1.7425,
      "step": 6580
    },
    {
      "epoch": 1.7439213999801515,
      "grad_norm": 1.1469954252243042,
      "learning_rate": 2.5668166181529508e-05,
      "loss": 1.7028,
      "step": 6590
    },
    {
      "epoch": 1.7465678652949155,
      "grad_norm": 0.7515109777450562,
      "learning_rate": 2.5403545911616832e-05,
      "loss": 1.7159,
      "step": 6600
    },
    {
      "epoch": 1.7492143306096795,
      "grad_norm": 0.6740395426750183,
      "learning_rate": 2.5138925641704154e-05,
      "loss": 1.7314,
      "step": 6610
    },
    {
      "epoch": 1.7518607959244434,
      "grad_norm": 0.6940569877624512,
      "learning_rate": 2.4874305371791482e-05,
      "loss": 1.7843,
      "step": 6620
    },
    {
      "epoch": 1.7545072612392074,
      "grad_norm": 0.6914936900138855,
      "learning_rate": 2.4609685101878804e-05,
      "loss": 1.6719,
      "step": 6630
    },
    {
      "epoch": 1.7571537265539714,
      "grad_norm": 0.9893372058868408,
      "learning_rate": 2.434506483196613e-05,
      "loss": 1.6722,
      "step": 6640
    },
    {
      "epoch": 1.7598001918687354,
      "grad_norm": 0.8872423768043518,
      "learning_rate": 2.4080444562053457e-05,
      "loss": 1.739,
      "step": 6650
    },
    {
      "epoch": 1.7624466571834994,
      "grad_norm": 0.7888092398643494,
      "learning_rate": 2.3815824292140778e-05,
      "loss": 1.6896,
      "step": 6660
    },
    {
      "epoch": 1.7650931224982633,
      "grad_norm": 0.8698133826255798,
      "learning_rate": 2.3551204022228103e-05,
      "loss": 1.7374,
      "step": 6670
    },
    {
      "epoch": 1.7677395878130273,
      "grad_norm": 0.9809449911117554,
      "learning_rate": 2.3286583752315428e-05,
      "loss": 1.7319,
      "step": 6680
    },
    {
      "epoch": 1.7703860531277913,
      "grad_norm": 0.8996221423149109,
      "learning_rate": 2.3021963482402753e-05,
      "loss": 1.7456,
      "step": 6690
    },
    {
      "epoch": 1.7730325184425553,
      "grad_norm": 0.7798066735267639,
      "learning_rate": 2.2757343212490077e-05,
      "loss": 1.6853,
      "step": 6700
    },
    {
      "epoch": 1.7756789837573193,
      "grad_norm": 0.8211212754249573,
      "learning_rate": 2.2492722942577402e-05,
      "loss": 1.7688,
      "step": 6710
    },
    {
      "epoch": 1.7783254490720832,
      "grad_norm": 0.8986272811889648,
      "learning_rate": 2.2228102672664727e-05,
      "loss": 1.632,
      "step": 6720
    },
    {
      "epoch": 1.7809719143868472,
      "grad_norm": 0.8002308011054993,
      "learning_rate": 2.1963482402752052e-05,
      "loss": 1.6908,
      "step": 6730
    },
    {
      "epoch": 1.7836183797016112,
      "grad_norm": 0.8013837337493896,
      "learning_rate": 2.1698862132839377e-05,
      "loss": 1.6551,
      "step": 6740
    },
    {
      "epoch": 1.7862648450163752,
      "grad_norm": 0.7677279710769653,
      "learning_rate": 2.14342418629267e-05,
      "loss": 1.6602,
      "step": 6750
    },
    {
      "epoch": 1.788911310331139,
      "grad_norm": 0.796776533126831,
      "learning_rate": 2.1169621593014023e-05,
      "loss": 1.6335,
      "step": 6760
    },
    {
      "epoch": 1.791557775645903,
      "grad_norm": 0.8175238966941833,
      "learning_rate": 2.090500132310135e-05,
      "loss": 1.7723,
      "step": 6770
    },
    {
      "epoch": 1.7942042409606669,
      "grad_norm": 0.6722047328948975,
      "learning_rate": 2.0640381053188676e-05,
      "loss": 1.7121,
      "step": 6780
    },
    {
      "epoch": 1.7968507062754309,
      "grad_norm": 0.6892452836036682,
      "learning_rate": 2.0375760783275998e-05,
      "loss": 1.7918,
      "step": 6790
    },
    {
      "epoch": 1.7994971715901948,
      "grad_norm": 0.8291841149330139,
      "learning_rate": 2.0111140513363326e-05,
      "loss": 1.7583,
      "step": 6800
    },
    {
      "epoch": 1.8021436369049588,
      "grad_norm": 0.7058987021446228,
      "learning_rate": 1.984652024345065e-05,
      "loss": 1.6155,
      "step": 6810
    },
    {
      "epoch": 1.8047901022197228,
      "grad_norm": 0.8830570578575134,
      "learning_rate": 1.9581899973537972e-05,
      "loss": 1.7872,
      "step": 6820
    },
    {
      "epoch": 1.8074365675344868,
      "grad_norm": 0.8404698967933655,
      "learning_rate": 1.93172797036253e-05,
      "loss": 1.6803,
      "step": 6830
    },
    {
      "epoch": 1.8100830328492508,
      "grad_norm": 0.7749622464179993,
      "learning_rate": 1.9052659433712625e-05,
      "loss": 1.6654,
      "step": 6840
    },
    {
      "epoch": 1.8127294981640147,
      "grad_norm": 0.7139227390289307,
      "learning_rate": 1.8788039163799947e-05,
      "loss": 1.7086,
      "step": 6850
    },
    {
      "epoch": 1.8153759634787785,
      "grad_norm": 0.8100466728210449,
      "learning_rate": 1.8523418893887275e-05,
      "loss": 1.6922,
      "step": 6860
    },
    {
      "epoch": 1.8180224287935425,
      "grad_norm": 0.8033730387687683,
      "learning_rate": 1.8258798623974596e-05,
      "loss": 1.6363,
      "step": 6870
    },
    {
      "epoch": 1.8206688941083065,
      "grad_norm": 0.9996386170387268,
      "learning_rate": 1.799417835406192e-05,
      "loss": 1.6928,
      "step": 6880
    },
    {
      "epoch": 1.8233153594230704,
      "grad_norm": 0.9657056331634521,
      "learning_rate": 1.7729558084149246e-05,
      "loss": 1.6979,
      "step": 6890
    },
    {
      "epoch": 1.8259618247378344,
      "grad_norm": 0.7573776841163635,
      "learning_rate": 1.746493781423657e-05,
      "loss": 1.7015,
      "step": 6900
    },
    {
      "epoch": 1.8286082900525984,
      "grad_norm": 0.8917462229728699,
      "learning_rate": 1.7200317544323896e-05,
      "loss": 1.8078,
      "step": 6910
    },
    {
      "epoch": 1.8312547553673624,
      "grad_norm": 0.9035374522209167,
      "learning_rate": 1.693569727441122e-05,
      "loss": 1.7548,
      "step": 6920
    },
    {
      "epoch": 1.8339012206821264,
      "grad_norm": 0.6712377667427063,
      "learning_rate": 1.6671077004498545e-05,
      "loss": 1.6946,
      "step": 6930
    },
    {
      "epoch": 1.8365476859968903,
      "grad_norm": 0.8568627238273621,
      "learning_rate": 1.640645673458587e-05,
      "loss": 1.6793,
      "step": 6940
    },
    {
      "epoch": 1.8391941513116543,
      "grad_norm": 0.7638144493103027,
      "learning_rate": 1.6141836464673195e-05,
      "loss": 1.7036,
      "step": 6950
    },
    {
      "epoch": 1.8418406166264183,
      "grad_norm": 0.676226794719696,
      "learning_rate": 1.587721619476052e-05,
      "loss": 1.758,
      "step": 6960
    },
    {
      "epoch": 1.8444870819411823,
      "grad_norm": 0.8127098083496094,
      "learning_rate": 1.5612595924847845e-05,
      "loss": 1.8154,
      "step": 6970
    },
    {
      "epoch": 1.8471335472559463,
      "grad_norm": 0.7811220288276672,
      "learning_rate": 1.5347975654935166e-05,
      "loss": 1.6433,
      "step": 6980
    },
    {
      "epoch": 1.8497800125707102,
      "grad_norm": 0.9590850472450256,
      "learning_rate": 1.5083355385022493e-05,
      "loss": 1.74,
      "step": 6990
    },
    {
      "epoch": 1.8524264778854742,
      "grad_norm": 0.9388757348060608,
      "learning_rate": 1.481873511510982e-05,
      "loss": 1.7392,
      "step": 7000
    },
    {
      "epoch": 1.8550729432002382,
      "grad_norm": 0.7623320817947388,
      "learning_rate": 1.4554114845197142e-05,
      "loss": 1.715,
      "step": 7010
    },
    {
      "epoch": 1.8577194085150022,
      "grad_norm": 0.8499939441680908,
      "learning_rate": 1.4289494575284467e-05,
      "loss": 1.7238,
      "step": 7020
    },
    {
      "epoch": 1.8603658738297661,
      "grad_norm": 0.6687272191047668,
      "learning_rate": 1.4024874305371794e-05,
      "loss": 1.7554,
      "step": 7030
    },
    {
      "epoch": 1.8630123391445301,
      "grad_norm": 0.8862444758415222,
      "learning_rate": 1.3760254035459117e-05,
      "loss": 1.7198,
      "step": 7040
    },
    {
      "epoch": 1.865658804459294,
      "grad_norm": 0.862343966960907,
      "learning_rate": 1.3495633765546442e-05,
      "loss": 1.6981,
      "step": 7050
    },
    {
      "epoch": 1.868305269774058,
      "grad_norm": 0.684289813041687,
      "learning_rate": 1.3231013495633765e-05,
      "loss": 1.6471,
      "step": 7060
    },
    {
      "epoch": 1.870951735088822,
      "grad_norm": 0.6544992327690125,
      "learning_rate": 1.2966393225721091e-05,
      "loss": 1.7424,
      "step": 7070
    },
    {
      "epoch": 1.873598200403586,
      "grad_norm": 0.8344442844390869,
      "learning_rate": 1.2701772955808416e-05,
      "loss": 1.6665,
      "step": 7080
    },
    {
      "epoch": 1.87624466571835,
      "grad_norm": 0.7635895013809204,
      "learning_rate": 1.2437152685895741e-05,
      "loss": 1.7208,
      "step": 7090
    },
    {
      "epoch": 1.878891131033114,
      "grad_norm": 0.7510536909103394,
      "learning_rate": 1.2172532415983064e-05,
      "loss": 1.7281,
      "step": 7100
    },
    {
      "epoch": 1.881537596347878,
      "grad_norm": 0.721286952495575,
      "learning_rate": 1.1907912146070389e-05,
      "loss": 1.7837,
      "step": 7110
    },
    {
      "epoch": 1.884184061662642,
      "grad_norm": 0.7948721647262573,
      "learning_rate": 1.1643291876157714e-05,
      "loss": 1.7203,
      "step": 7120
    },
    {
      "epoch": 1.886830526977406,
      "grad_norm": 0.7651481628417969,
      "learning_rate": 1.1378671606245039e-05,
      "loss": 1.7153,
      "step": 7130
    },
    {
      "epoch": 1.88947699229217,
      "grad_norm": 0.6926605701446533,
      "learning_rate": 1.1114051336332364e-05,
      "loss": 1.6723,
      "step": 7140
    },
    {
      "epoch": 1.892123457606934,
      "grad_norm": 0.756273627281189,
      "learning_rate": 1.0849431066419688e-05,
      "loss": 1.7577,
      "step": 7150
    },
    {
      "epoch": 1.8947699229216977,
      "grad_norm": 0.9049482345581055,
      "learning_rate": 1.0584810796507012e-05,
      "loss": 1.8171,
      "step": 7160
    },
    {
      "epoch": 1.8974163882364616,
      "grad_norm": 0.6803799271583557,
      "learning_rate": 1.0320190526594338e-05,
      "loss": 1.7014,
      "step": 7170
    },
    {
      "epoch": 1.9000628535512256,
      "grad_norm": 0.7376788258552551,
      "learning_rate": 1.0055570256681663e-05,
      "loss": 1.7472,
      "step": 7180
    },
    {
      "epoch": 1.9027093188659896,
      "grad_norm": 0.7276326417922974,
      "learning_rate": 9.790949986768986e-06,
      "loss": 1.7124,
      "step": 7190
    },
    {
      "epoch": 1.9053557841807536,
      "grad_norm": 1.0390647649765015,
      "learning_rate": 9.526329716856313e-06,
      "loss": 1.7665,
      "step": 7200
    },
    {
      "epoch": 1.9080022494955176,
      "grad_norm": 0.75986248254776,
      "learning_rate": 9.261709446943637e-06,
      "loss": 1.6906,
      "step": 7210
    },
    {
      "epoch": 1.9106487148102815,
      "grad_norm": 1.2098281383514404,
      "learning_rate": 8.99708917703096e-06,
      "loss": 1.7749,
      "step": 7220
    },
    {
      "epoch": 1.9132951801250455,
      "grad_norm": 0.800700843334198,
      "learning_rate": 8.732468907118285e-06,
      "loss": 1.7398,
      "step": 7230
    },
    {
      "epoch": 1.9159416454398095,
      "grad_norm": 0.7805551290512085,
      "learning_rate": 8.46784863720561e-06,
      "loss": 1.8165,
      "step": 7240
    },
    {
      "epoch": 1.9185881107545735,
      "grad_norm": 0.8370581865310669,
      "learning_rate": 8.203228367292935e-06,
      "loss": 1.714,
      "step": 7250
    },
    {
      "epoch": 1.9212345760693372,
      "grad_norm": 0.9502409100532532,
      "learning_rate": 7.93860809738026e-06,
      "loss": 1.7763,
      "step": 7260
    },
    {
      "epoch": 1.9238810413841012,
      "grad_norm": 1.067305564880371,
      "learning_rate": 7.673987827467583e-06,
      "loss": 1.7167,
      "step": 7270
    },
    {
      "epoch": 1.9265275066988652,
      "grad_norm": 0.7753320336341858,
      "learning_rate": 7.40936755755491e-06,
      "loss": 1.79,
      "step": 7280
    },
    {
      "epoch": 1.9291739720136292,
      "grad_norm": 1.1916959285736084,
      "learning_rate": 7.144747287642234e-06,
      "loss": 1.7297,
      "step": 7290
    },
    {
      "epoch": 1.9318204373283931,
      "grad_norm": 0.9240244626998901,
      "learning_rate": 6.8801270177295584e-06,
      "loss": 1.6728,
      "step": 7300
    },
    {
      "epoch": 1.9344669026431571,
      "grad_norm": 0.8192377686500549,
      "learning_rate": 6.615506747816882e-06,
      "loss": 1.6577,
      "step": 7310
    },
    {
      "epoch": 1.937113367957921,
      "grad_norm": 0.6417976021766663,
      "learning_rate": 6.350886477904208e-06,
      "loss": 1.6867,
      "step": 7320
    },
    {
      "epoch": 1.939759833272685,
      "grad_norm": 0.6365160346031189,
      "learning_rate": 6.086266207991532e-06,
      "loss": 1.7472,
      "step": 7330
    },
    {
      "epoch": 1.942406298587449,
      "grad_norm": 0.975855827331543,
      "learning_rate": 5.821645938078857e-06,
      "loss": 1.588,
      "step": 7340
    },
    {
      "epoch": 1.945052763902213,
      "grad_norm": 1.006410002708435,
      "learning_rate": 5.557025668166182e-06,
      "loss": 1.7298,
      "step": 7350
    },
    {
      "epoch": 1.947699229216977,
      "grad_norm": 0.9076945781707764,
      "learning_rate": 5.292405398253506e-06,
      "loss": 1.7288,
      "step": 7360
    },
    {
      "epoch": 1.950345694531741,
      "grad_norm": 0.775295078754425,
      "learning_rate": 5.0277851283408315e-06,
      "loss": 1.7258,
      "step": 7370
    },
    {
      "epoch": 1.952992159846505,
      "grad_norm": 0.7006844282150269,
      "learning_rate": 4.763164858428156e-06,
      "loss": 1.6843,
      "step": 7380
    },
    {
      "epoch": 1.955638625161269,
      "grad_norm": 0.8233301043510437,
      "learning_rate": 4.49854458851548e-06,
      "loss": 1.7509,
      "step": 7390
    },
    {
      "epoch": 1.958285090476033,
      "grad_norm": 0.8805111646652222,
      "learning_rate": 4.233924318602805e-06,
      "loss": 1.7275,
      "step": 7400
    },
    {
      "epoch": 1.960931555790797,
      "grad_norm": 1.0973923206329346,
      "learning_rate": 3.96930404869013e-06,
      "loss": 1.6558,
      "step": 7410
    },
    {
      "epoch": 1.963578021105561,
      "grad_norm": 0.7489078640937805,
      "learning_rate": 3.704683778777455e-06,
      "loss": 1.7126,
      "step": 7420
    },
    {
      "epoch": 1.9662244864203249,
      "grad_norm": 0.6883534789085388,
      "learning_rate": 3.4400635088647792e-06,
      "loss": 1.6512,
      "step": 7430
    },
    {
      "epoch": 1.9688709517350889,
      "grad_norm": 0.8963738679885864,
      "learning_rate": 3.175443238952104e-06,
      "loss": 1.6384,
      "step": 7440
    },
    {
      "epoch": 1.9715174170498528,
      "grad_norm": 0.9963681697845459,
      "learning_rate": 2.9108229690394285e-06,
      "loss": 1.6501,
      "step": 7450
    },
    {
      "epoch": 1.9741638823646168,
      "grad_norm": 0.8519454002380371,
      "learning_rate": 2.646202699126753e-06,
      "loss": 1.717,
      "step": 7460
    },
    {
      "epoch": 1.9768103476793808,
      "grad_norm": 0.7579869627952576,
      "learning_rate": 2.381582429214078e-06,
      "loss": 1.7791,
      "step": 7470
    },
    {
      "epoch": 1.9794568129941448,
      "grad_norm": 1.0481462478637695,
      "learning_rate": 2.1169621593014026e-06,
      "loss": 1.8262,
      "step": 7480
    },
    {
      "epoch": 1.9821032783089088,
      "grad_norm": 0.802913248538971,
      "learning_rate": 1.8523418893887274e-06,
      "loss": 1.7298,
      "step": 7490
    },
    {
      "epoch": 1.9847497436236727,
      "grad_norm": 0.8490570783615112,
      "learning_rate": 1.587721619476052e-06,
      "loss": 1.737,
      "step": 7500
    },
    {
      "epoch": 1.9873962089384367,
      "grad_norm": 0.8022938966751099,
      "learning_rate": 1.3231013495633764e-06,
      "loss": 1.732,
      "step": 7510
    },
    {
      "epoch": 1.9900426742532007,
      "grad_norm": 0.7471645474433899,
      "learning_rate": 1.0584810796507013e-06,
      "loss": 1.7549,
      "step": 7520
    },
    {
      "epoch": 1.9926891395679647,
      "grad_norm": 0.7970514893531799,
      "learning_rate": 7.93860809738026e-07,
      "loss": 1.7007,
      "step": 7530
    },
    {
      "epoch": 1.9953356048827287,
      "grad_norm": 0.6946226358413696,
      "learning_rate": 5.292405398253506e-07,
      "loss": 1.6463,
      "step": 7540
    },
    {
      "epoch": 1.9979820701974926,
      "grad_norm": 0.704979419708252,
      "learning_rate": 2.646202699126753e-07,
      "loss": 1.6914,
      "step": 7550
    }
  ],
  "logging_steps": 10,
  "max_steps": 7558,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.3971720287174656e+16,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
